{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Air Quality PM2.5 Prediction - Model Training & Evaluation\n",
    "\n",
    "\n",
    "## Objectives\n",
    "1. Load processed training and test data\n",
    "2. Train baseline models (Linear Regression, Ridge, Lasso)\n",
    "3. Train tree-based models (Random Forest, Gradient Boosting, XGBoost)\n",
    "4. Train advanced models (LightGBM, CatBoost)\n",
    "5. Perform hyperparameter tuning\n",
    "6. Compare model performance\n",
    "7. Analyze residuals and predictions\n",
    "8. Select and save best model\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **RMSE** (Root Mean Squared Error) - Lower is better\n",
    "- **MAE** (Mean Absolute Error) - Lower is better\n",
    "- **R²** (R-squared) - Higher is better (max 1.0)\n",
    "- **MAPE** (Mean Absolute Percentage Error) - Lower is better\n",
    "\n",
    "## Models to Train\n",
    "1. Linear Regression (Baseline)\n",
    "2. Ridge Regression (L2 regularization)\n",
    "3. Lasso Regression (L1 regularization)\n",
    "4. ElasticNet (L1 + L2 regularization)\n",
    "5. Random Forest Regressor\n",
    "6. Gradient Boosting Regressor\n",
    "7. XGBoost Regressor\n",
    "8. LightGBM Regressor\n",
    "9. CatBoost Regressor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn - Preprocessing & Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Sklearn - Model Selection\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    KFold\n",
    ")\n",
    "\n",
    "# Sklearn - Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD PROCESSED DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = '../data/processed/'\n",
    "\n",
    "print(f\"\\n Loading data from: {DATA_PATH}\")\n",
    "\n",
    "# Load training data\n",
    "X_train = pd.read_csv(f'{DATA_PATH}X_train.csv')\n",
    "y_train = pd.read_csv(f'{DATA_PATH}y_train.csv').values.ravel()  # Flatten to 1D array\n",
    "y_train_original = pd.read_csv(f'{DATA_PATH}y_train_original.csv').values.ravel()\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(f'{DATA_PATH}X_test.csv')\n",
    "y_test = pd.read_csv(f'{DATA_PATH}y_test.csv').values.ravel()\n",
    "y_test_original = pd.read_csv(f'{DATA_PATH}y_test_original.csv').values.ravel()\n",
    "\n",
    "# Load metadata\n",
    "metadata_train = pd.read_csv(f'{DATA_PATH}metadata_train.csv')\n",
    "metadata_test = pd.read_csv(f'{DATA_PATH}metadata_test.csv')\n",
    "\n",
    "print(f\"\\n Data loaded successfully!\")\n",
    "\n",
    "print(f\"\\n Dataset Shapes:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training Features (X_train): {X_train.shape}\")\n",
    "print(f\"Training Target (y_train): {y_train.shape}\")\n",
    "print(f\"Test Features (X_test): {X_test.shape}\")\n",
    "print(f\"Test Target (y_test): {y_test.shape}\")\n",
    "\n",
    "print(f\"\\n Target Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training Target (log-transformed):\")\n",
    "print(f\"   Mean: {y_train.mean():.3f}\")\n",
    "print(f\"   Std: {y_train.std():.3f}\")\n",
    "print(f\"   Min: {y_train.min():.3f}\")\n",
    "print(f\"   Max: {y_train.max():.3f}\")\n",
    "\n",
    "print(f\"\\nTraining Target (original scale):\")\n",
    "print(f\"   Mean: {y_train_original.mean():.2f} μg/m³\")\n",
    "print(f\"   Std: {y_train_original.std():.2f} μg/m³\")\n",
    "print(f\"   Min: {y_train_original.min():.2f} μg/m³\")\n",
    "print(f\"   Max: {y_train_original.max():.2f} μg/m³\")\n",
    "\n",
    "print(f\"\\n Feature Information:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nFirst 10 features:\")\n",
    "for i, feature in enumerate(X_train.columns[:10], 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "if len(X_train.columns) > 10:\n",
    "    print(f\"   ... and {len(X_train.columns) - 10} more features\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_train = X_train.isnull().sum().sum()\n",
    "missing_test = X_test.isnull().sum().sum()\n",
    "\n",
    "if missing_train > 0 or missing_test > 0:\n",
    "    print(f\"\\n Warning: Missing values detected!\")\n",
    "    print(f\"   Training: {missing_train}\")\n",
    "    print(f\"   Test: {missing_test}\")\n",
    "else:\n",
    "    print(f\"\\n No missing values!\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n Sample Data (First 5 rows):\")\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE SCALING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Standardizing features using StandardScaler...\")\n",
    "print(f\"   (Mean = 0, Std = 1)\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\n Feature scaling completed!\")\n",
    "\n",
    "print(f\"\\n Scaled Features Statistics:\")\n",
    "print(f\"   Training mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Training std: {X_train_scaled.std().mean():.6f}\")\n",
    "print(f\"   Test mean: {X_test_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Test std: {X_test_scaled.std().mean():.6f}\")\n",
    "\n",
    "# Visualize scaling effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Before scaling - first feature\n",
    "feature_name = X_train.columns[0]\n",
    "axes[0].hist(X_train[feature_name], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0].set_xlabel(feature_name[:40])\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'Before Scaling: {feature_name[:30]}...', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After scaling - same feature\n",
    "axes[1].hist(X_train_scaled[feature_name], bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "axes[1].set_xlabel(f'{feature_name[:40]} (scaled)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'After Scaling: {feature_name[:30]}...', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEFINING EVALUATION FUNCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    train_mape = np.mean(np.abs((y_train - y_train_pred) / (y_train + 1e-10))) * 100\n",
    "    test_mape = np.mean(np.abs((y_test - y_test_pred) / (y_test + 1e-10))) * 100\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Train_MAPE': train_mape,\n",
    "        'Test_MAPE': test_mape,\n",
    "        'Predictions_Train': y_train_pred,\n",
    "        'Predictions_Test': y_test_pred\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_evaluation(results):\n",
    "    \"\"\"\n",
    "    Print evaluation metrics in a formatted way\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODEL: {results['Model']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n RMSE (Root Mean Squared Error):\")\n",
    "    print(f\"   Training:   {results['Train_RMSE']:.4f}\")\n",
    "    print(f\"   Test:       {results['Test_RMSE']:.4f}\")\n",
    "    print(f\"   Difference: {abs(results['Train_RMSE'] - results['Test_RMSE']):.4f}\")\n",
    "    \n",
    "    print(f\"\\n MAE (Mean Absolute Error):\")\n",
    "    print(f\"   Training:   {results['Train_MAE']:.4f}\")\n",
    "    print(f\"   Test:       {results['Test_MAE']:.4f}\")\n",
    "    print(f\"   Difference: {abs(results['Train_MAE'] - results['Test_MAE']):.4f}\")\n",
    "    \n",
    "    print(f\"\\n R² (R-squared):\")\n",
    "    print(f\"   Training:   {results['Train_R2']:.4f}\")\n",
    "    print(f\"   Test:       {results['Test_R2']:.4f}\")\n",
    "    print(f\"   Difference: {abs(results['Train_R2'] - results['Test_R2']):.4f}\")\n",
    "    \n",
    "    print(f\"\\n MAPE (Mean Absolute Percentage Error):\")\n",
    "    print(f\"   Training:   {results['Train_MAPE']:.2f}%\")\n",
    "    print(f\"   Test:       {results['Test_MAPE']:.2f}%\")\n",
    "    \n",
    "    # Overfitting check\n",
    "    if results['Train_R2'] - results['Test_R2'] > 0.1:\n",
    "        print(f\"\\n Warning: Possible overfitting detected!\")\n",
    "        print(f\"   Train R² is {results['Train_R2'] - results['Test_R2']:.4f} higher than Test R²\")\n",
    "    else:\n",
    "        print(f\"\\n Model generalizes well!\")\n",
    "\n",
    "print(\"\\n Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE MODEL 1: LINEAR REGRESSION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n Training Model 1: Linear Regression...\")\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_model(\n",
    "    lr_model, \n",
    "    X_train_scaled, \n",
    "    X_test_scaled, \n",
    "    y_train, \n",
    "    y_test, \n",
    "    model_name=\"Linear Regression\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(lr_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(lr_results)\n",
    "\n",
    "print(f\"\\n Linear Regression completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE MODEL 2: RIDGE REGRESSION (L2 Regularization)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RIDGE REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 2: Ridge Regression...\")\n",
    "\n",
    "# Train Ridge Regression with default alpha\n",
    "ridge_model = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ridge_results = evaluate_model(\n",
    "    ridge_model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"Ridge Regression\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(ridge_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(ridge_results)\n",
    "\n",
    "print(f\"\\n Ridge Regression completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE MODEL 3: LASSO REGRESSION (L1 Regularization)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LASSO REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTraining Model 3: Lasso Regression...\")\n",
    "\n",
    "# Train Lasso Regression with default alpha\n",
    "lasso_model = Lasso(alpha=0.1, random_state=RANDOM_STATE, max_iter=5000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lasso_results = evaluate_model(\n",
    "    lasso_model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"Lasso Regression\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(lasso_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(lasso_results)\n",
    "\n",
    "# Check feature selection (Lasso can zero out coefficients)\n",
    "non_zero_coefs = np.sum(lasso_model.coef_ != 0)\n",
    "print(f\"\\n Feature Selection:\")\n",
    "print(f\"   Features with non-zero coefficients: {non_zero_coefs}/{len(lasso_model.coef_)}\")\n",
    "print(f\"   Features zeroed out: {len(lasso_model.coef_) - non_zero_coefs}\")\n",
    "\n",
    "print(f\"\\n Lasso Regression completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BASELINE MODEL 4: ELASTICNET REGRESSION (L1 + L2)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ELASTICNET REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 4: ElasticNet Regression...\")\n",
    "\n",
    "# Train ElasticNet (combines Ridge and Lasso)\n",
    "elasticnet_model = ElasticNet(\n",
    "    alpha=0.1, \n",
    "    l1_ratio=0.5,  # 0.5 means equal mix of L1 and L2\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=5000\n",
    ")\n",
    "elasticnet_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "elasticnet_results = evaluate_model(\n",
    "    elasticnet_model,\n",
    "    X_train_scaled,\n",
    "    X_test_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"ElasticNet Regression\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(elasticnet_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(elasticnet_results)\n",
    "\n",
    "print(f\"\\n ElasticNet Regression completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE BASELINE MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "baseline_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['Model'],\n",
    "        'Test_RMSE': result['Test_RMSE'],\n",
    "        'Test_MAE': result['Test_MAE'],\n",
    "        'Test_R2': result['Test_R2'],\n",
    "        'Test_MAPE': result['Test_MAPE']\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(f\"\\n Baseline Models Performance:\")\n",
    "display(baseline_comparison.sort_values('Test_R2', ascending=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Test RMSE\n",
    "axes[0, 0].bar(baseline_comparison['Model'], baseline_comparison['Test_RMSE'],\n",
    "              color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_ylabel('RMSE (Lower is Better)')\n",
    "axes[0, 0].set_title('Test RMSE Comparison', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Test MAE\n",
    "axes[0, 1].bar(baseline_comparison['Model'], baseline_comparison['Test_MAE'],\n",
    "              color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_ylabel('MAE (Lower is Better)')\n",
    "axes[0, 1].set_title('Test MAE Comparison', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Test R²\n",
    "axes[1, 0].bar(baseline_comparison['Model'], baseline_comparison['Test_R2'],\n",
    "              color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('R² (Higher is Better)')\n",
    "axes[1, 0].set_title('Test R² Comparison', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "axes[1, 0].axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='R²=0.5')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Test MAPE\n",
    "axes[1, 1].bar(baseline_comparison['Model'], baseline_comparison['Test_MAPE'],\n",
    "              color='plum', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_ylabel('MAPE % (Lower is Better)')\n",
    "axes[1, 1].set_title('Test MAPE Comparison', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best baseline model\n",
    "best_baseline = baseline_comparison.loc[baseline_comparison['Test_R2'].idxmax()]\n",
    "print(f\"\\n Best Baseline Model: {best_baseline['Model']}\")\n",
    "print(f\"   Test R²: {best_baseline['Test_R2']:.4f}\")\n",
    "print(f\"   Test RMSE: {best_baseline['Test_RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TREE-BASED MODEL 1: RANDOM FOREST\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING TREE-BASED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 5: Random Forest Regressor...\")\n",
    "print(f\"   (This may take a few minutes...)\")\n",
    "\n",
    "# Train Random Forest (use unscaled features for tree models)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_model(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"Random Forest\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(rf_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(rf_results)\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n Top 10 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "for i, row in enumerate(feature_importance.head(10).itertuples(), 1):\n",
    "    print(f\"   {i}. {row.Feature[:50]}: {row.Importance:.4f}\")\n",
    "\n",
    "print(f\"\\n Random Forest completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TREE-BASED MODEL 2: GRADIENT BOOSTING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING GRADIENT BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 6: Gradient Boosting Regressor...\")\n",
    "print(f\"   (This may take a few minutes...)\")\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "gb_results = evaluate_model(\n",
    "    gb_model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(gb_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(gb_results)\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n Top 10 Most Important Features:\")\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "for i, row in enumerate(feature_importance_gb.head(10).itertuples(), 1):\n",
    "    print(f\"   {i}. {row.Feature[:50]}: {row.Importance:.4f}\")\n",
    "\n",
    "print(f\"\\n Gradient Boosting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ============================================\n",
    "# TREE-BASED MODEL 3: XGBOOST\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 7: XGBoost Regressor...\")\n",
    "print(f\"   (This may take a few minutes...)\")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "xgb_results = evaluate_model(\n",
    "    xgb_model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"XGBoost\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(xgb_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(xgb_results)\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n Top 10 Most Important Features:\")\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "for i, row in enumerate(feature_importance_xgb.head(10).itertuples(), 1):\n",
    "    print(f\"   {i}. {row.Feature[:50]}: {row.Importance:.4f}\")\n",
    "\n",
    "print(f\"\\n XGBoost completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADVANCED MODEL: LIGHTGBM\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Training Model 8: LightGBM Regressor...\")\n",
    "print(f\"   (This may take a few minutes...)\")\n",
    "\n",
    "# Train LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lgbm_results = evaluate_model(\n",
    "    lgbm_model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"LightGBM\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_evaluation(lgbm_results)\n",
    "\n",
    "# Store results\n",
    "all_results.append(lgbm_results)\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n Top 10 Most Important Features:\")\n",
    "feature_importance_lgbm = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': lgbm_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "for i, row in enumerate(feature_importance_lgbm.head(10).itertuples(), 1):\n",
    "    print(f\"   {i}. {row.Feature[:50]}: {row.Importance:.4f}\")\n",
    "\n",
    "print(f\"\\n LightGBM completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE ALL MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive comparison dataframe\n",
    "model_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['Model'],\n",
    "        'Train_RMSE': result['Train_RMSE'],\n",
    "        'Test_RMSE': result['Test_RMSE'],\n",
    "        'Train_MAE': result['Train_MAE'],\n",
    "        'Test_MAE': result['Test_MAE'],\n",
    "        'Train_R2': result['Train_R2'],\n",
    "        'Test_R2': result['Test_R2'],\n",
    "        'Train_MAPE': result['Train_MAPE'],\n",
    "        'Test_MAPE': result['Test_MAPE'],\n",
    "        'Overfit_Gap': result['Train_R2'] - result['Test_R2']\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "# Sort by Test R² (descending)\n",
    "model_comparison = model_comparison.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(f\"\\n Complete Model Performance Table:\")\n",
    "print(f\"{'='*100}\")\n",
    "display(model_comparison)\n",
    "\n",
    "# Visualize comprehensive comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Test RMSE\n",
    "axes[0, 0].barh(range(len(model_comparison)), model_comparison['Test_RMSE'],\n",
    "               color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(model_comparison)))\n",
    "axes[0, 0].set_yticklabels(model_comparison['Model'])\n",
    "axes[0, 0].set_xlabel('RMSE')\n",
    "axes[0, 0].set_title('Test RMSE (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Test R²\n",
    "axes[0, 1].barh(range(len(model_comparison)), model_comparison['Test_R2'],\n",
    "               color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_yticks(range(len(model_comparison)))\n",
    "axes[0, 1].set_yticklabels(model_comparison['Model'])\n",
    "axes[0, 1].set_xlabel('R²')\n",
    "axes[0, 1].set_title('Test R² (Higher is Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "axes[0, 1].axvline(x=0.8, color='red', linestyle='--', linewidth=1, label='R²=0.8')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Test MAE\n",
    "axes[0, 2].barh(range(len(model_comparison)), model_comparison['Test_MAE'],\n",
    "               color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_yticks(range(len(model_comparison)))\n",
    "axes[0, 2].set_yticklabels(model_comparison['Model'])\n",
    "axes[0, 2].set_xlabel('MAE')\n",
    "axes[0, 2].set_title('Test MAE (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "axes[0, 2].invert_yaxis()\n",
    "axes[0, 2].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Train vs Test R² (Overfitting check)\n",
    "x = np.arange(len(model_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, model_comparison['Train_R2'], width,\n",
    "              label='Train R²', color='lightblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].bar(x + width/2, model_comparison['Test_R2'], width,\n",
    "              label='Test R²', color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(model_comparison['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('R²')\n",
    "axes[1, 0].set_title('Train vs Test R² (Overfitting Check)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Overfitting Gap\n",
    "axes[1, 1].barh(range(len(model_comparison)), model_comparison['Overfit_Gap'],\n",
    "               color='purple', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(model_comparison)))\n",
    "axes[1, 1].set_yticklabels(model_comparison['Model'])\n",
    "axes[1, 1].set_xlabel('Overfitting Gap (Train R² - Test R²)')\n",
    "axes[1, 1].set_title('Overfitting Analysis', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].axvline(x=0.1, color='red', linestyle='--', linewidth=1, label='Gap=0.1')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 6. Test MAPE\n",
    "axes[1, 2].barh(range(len(model_comparison)), model_comparison['Test_MAPE'],\n",
    "               color='plum', edgecolor='black', alpha=0.7)\n",
    "axes[1, 2].set_yticks(range(len(model_comparison)))\n",
    "axes[1, 2].set_yticklabels(model_comparison['Model'])\n",
    "axes[1, 2].set_xlabel('MAPE (%)')\n",
    "axes[1, 2].set_title('Test MAPE (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "axes[1, 2].invert_yaxis()\n",
    "axes[1, 2].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = model_comparison['Test_R2'].idxmax()\n",
    "best_model_name = model_comparison.loc[best_model_idx, 'Model']\n",
    "best_test_r2 = model_comparison.loc[best_model_idx, 'Test_R2']\n",
    "best_test_rmse = model_comparison.loc[best_model_idx, 'Test_RMSE']\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\" BEST MODEL: {best_model_name}\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"   Test R²: {best_test_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {best_test_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {model_comparison.loc[best_model_idx, 'Test_MAE']:.4f}\")\n",
    "print(f\"   Test MAPE: {model_comparison.loc[best_model_idx, 'Test_MAPE']:.2f}%\")\n",
    "print(f\"   Overfitting Gap: {model_comparison.loc[best_model_idx, 'Overfit_Gap']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HYPERPARAMETER TUNING WITH OPTUNA\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING WITH OPTUNA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install optuna if not already installed\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    print(f\" Optuna version: {optuna.__version__}\")\n",
    "except:\n",
    "    print(\"Installing Optuna...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install optuna\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    print(f\" Optuna installed: {optuna.__version__}\")\n",
    "\n",
    "# Suppress optuna logs for cleaner output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Get top 3 models by Test R²\n",
    "top_3_models = model_comparison.head(3)['Model'].tolist()\n",
    "\n",
    "print(f\"\\n Tuning top 3 models with Optuna (50 trials each):\")\n",
    "for i, model in enumerate(top_3_models, 1):\n",
    "    print(f\"   {i}. {model}\")\n",
    "\n",
    "print(f\"\\n Optuna will automatically find optimal hyperparameters!\")\n",
    "\n",
    "# Store tuned results\n",
    "tuned_results = []\n",
    "tuned_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTUNA TUNE: XGBOOST\n",
    "# ============================================\n",
    "\n",
    "if 'XGBoost' in top_3_models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTUNA TUNING: XGBOOST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def objective_xgb(trial):\n",
    "        \"\"\"Objective function for XGBoost\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = XGBRegressor(**params)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2', n_jobs=-1)\n",
    "        \n",
    "        return scores.mean()\n",
    "    \n",
    "    print(f\"\\n Running Optuna optimization (50 trials)...\")\n",
    "    print(f\"   This will take a few minutes...\")\n",
    "    \n",
    "    # Create study\n",
    "    study_xgb = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study_xgb.optimize(objective_xgb, n_trials=10, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\n Optimization completed!\")\n",
    "    print(f\"   Best CV R²: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"   Number of trials: {len(study_xgb.trials)}\")\n",
    "    \n",
    "    print(f\"\\n Best hyperparameters:\")\n",
    "    for param, value in study_xgb.best_params.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    xgb_tuned = XGBRegressor(**study_xgb.best_params, random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
    "    xgb_tuned.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    xgb_tuned_results = evaluate_model(\n",
    "        xgb_tuned,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        model_name=\"XGBoost (Optuna)\"\n",
    "    )\n",
    "    \n",
    "    print_evaluation(xgb_tuned_results)\n",
    "    tuned_results.append(xgb_tuned_results)\n",
    "    tuned_models['XGBoost (Optuna)'] = xgb_tuned\n",
    "    \n",
    "    # Visualize optimization history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Optimization history\n",
    "    trial_numbers = [t.number for t in study_xgb.trials]\n",
    "    trial_values = [t.value for t in study_xgb.trials]\n",
    "    \n",
    "    axes[0].plot(trial_numbers, trial_values, 'o-', alpha=0.6, color='purple')\n",
    "    axes[0].axhline(y=study_xgb.best_value, color='red', linestyle='--', linewidth=2, label='Best')\n",
    "    axes[0].set_xlabel('Trial Number')\n",
    "    axes[0].set_ylabel('CV R²')\n",
    "    axes[0].set_title('XGBoost - Optimization History', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Parameter importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study_xgb)\n",
    "        params = list(importance.keys())[:10]  # Top 10\n",
    "        values = list(importance.values())[:10]\n",
    "        \n",
    "        axes[1].barh(params, values, color='plum', edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_xlabel('Importance')\n",
    "        axes[1].set_title('XGBoost - Top 10 Parameter Importance', fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3, axis='x')\n",
    "    except:\n",
    "        axes[1].text(0.5, 0.5, 'Parameter importance not available', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n XGBoost (Optuna) completed!\")\n",
    "else:\n",
    "    print(\"\\nXGBoost not in top 3, skipping tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTUNA TUNE: LIGHTGBM\n",
    "# ============================================\n",
    "\n",
    "if 'LightGBM' in top_3_models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTUNA TUNING: LIGHTGBM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def objective_lgbm(trial):\n",
    "        \"\"\"Objective function for LightGBM\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = LGBMRegressor(**params)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2', n_jobs=-1)\n",
    "        \n",
    "        return scores.mean()\n",
    "    \n",
    "    print(f\"\\n Running Optuna optimization (50 trials)...\")\n",
    "    print(f\"   This will take a few minutes...\")\n",
    "    \n",
    "    # Create study\n",
    "    study_lgbm = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study_lgbm.optimize(objective_lgbm, n_trials=10, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\n Optimization completed!\")\n",
    "    print(f\"   Best CV R²: {study_lgbm.best_value:.4f}\")\n",
    "    print(f\"   Number of trials: {len(study_lgbm.trials)}\")\n",
    "    \n",
    "    print(f\"\\n Best hyperparameters:\")\n",
    "    for param, value in study_lgbm.best_params.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    lgbm_tuned = LGBMRegressor(**study_lgbm.best_params, random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\n",
    "    lgbm_tuned.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    lgbm_tuned_results = evaluate_model(\n",
    "        lgbm_tuned,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        model_name=\"LightGBM (Optuna)\"\n",
    "    )\n",
    "    \n",
    "    print_evaluation(lgbm_tuned_results)\n",
    "    tuned_results.append(lgbm_tuned_results)\n",
    "    tuned_models['LightGBM (Optuna)'] = lgbm_tuned\n",
    "    \n",
    "    # Visualize optimization history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Optimization history\n",
    "    trial_numbers = [t.number for t in study_lgbm.trials]\n",
    "    trial_values = [t.value for t in study_lgbm.trials]\n",
    "    \n",
    "    axes[0].plot(trial_numbers, trial_values, 'o-', alpha=0.6, color='teal')\n",
    "    axes[0].axhline(y=study_lgbm.best_value, color='red', linestyle='--', linewidth=2, label='Best')\n",
    "    axes[0].set_xlabel('Trial Number')\n",
    "    axes[0].set_ylabel('CV R²')\n",
    "    axes[0].set_title('LightGBM - Optimization History', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Parameter importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study_lgbm)\n",
    "        params = list(importance.keys())[:10]  # Top 10\n",
    "        values = list(importance.values())[:10]\n",
    "        \n",
    "        axes[1].barh(params, values, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_xlabel('Importance')\n",
    "        axes[1].set_title('LightGBM - Top 10 Parameter Importance', fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3, axis='x')\n",
    "    except:\n",
    "        axes[1].text(0.5, 0.5, 'Parameter importance not available', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n LightGBM (Optuna) completed!\")\n",
    "else:\n",
    "    print(\"\\n LightGBM not in top 3, skipping tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE ORIGINAL VS TUNED MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORIGINAL VS TUNED MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all results (original + tuned)\n",
    "all_models_results = all_results + tuned_results\n",
    "\n",
    "# Create comparison dataframe\n",
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['Model'],\n",
    "        'Train_RMSE': result['Train_RMSE'],\n",
    "        'Test_RMSE': result['Test_RMSE'],\n",
    "        'Train_MAE': result['Train_MAE'],\n",
    "        'Test_MAE': result['Test_MAE'],\n",
    "        'Train_R2': result['Train_R2'],\n",
    "        'Test_R2': result['Test_R2'],\n",
    "        'Train_MAPE': result['Train_MAPE'],\n",
    "        'Test_MAPE': result['Test_MAPE'],\n",
    "        'Overfit_Gap': result['Train_R2'] - result['Test_R2']\n",
    "    }\n",
    "    for result in all_models_results\n",
    "])\n",
    "\n",
    "# Sort by Test R²\n",
    "final_comparison = final_comparison.sort_values('Test_R2', ascending=False)\n",
    "\n",
    "print(f\"\\n Final Model Rankings:\")\n",
    "print(f\"{'='*100}\")\n",
    "display(final_comparison)\n",
    "\n",
    "# Visualize improvement from tuning\n",
    "tuned_model_names = [r['Model'] for r in tuned_results]\n",
    "original_tuned_pairs = []\n",
    "\n",
    "for tuned_name in tuned_model_names:\n",
    "    # Extract original model name (remove Optuna/Tuned suffix)\n",
    "    if '(Optuna)' in tuned_name:\n",
    "        original_name = tuned_name.replace(' (Optuna)', '')\n",
    "    elif '(Tuned)' in tuned_name:\n",
    "        original_name = tuned_name.replace(' (Tuned)', '')\n",
    "    else:\n",
    "        original_name = tuned_name\n",
    "    \n",
    "    if original_name in [r['Model'] for r in all_results]:\n",
    "        original_score = final_comparison[final_comparison['Model'] == original_name]['Test_R2'].values\n",
    "        tuned_score = final_comparison[final_comparison['Model'] == tuned_name]['Test_R2'].values\n",
    "        \n",
    "        if len(original_score) > 0 and len(tuned_score) > 0:\n",
    "            original_tuned_pairs.append({\n",
    "                'Model': original_name,\n",
    "                'Original_R2': original_score[0],\n",
    "                'Tuned_R2': tuned_score[0],\n",
    "                'Improvement': tuned_score[0] - original_score[0],\n",
    "                'Improvement_Pct': ((tuned_score[0] - original_score[0]) / original_score[0]) * 100\n",
    "            })\n",
    "\n",
    "if len(original_tuned_pairs) > 0:\n",
    "    improvement_df = pd.DataFrame(original_tuned_pairs)\n",
    "    \n",
    "    print(f\"\\nImprovement from Hyperparameter Tuning (Optuna):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    display(improvement_df)\n",
    "    \n",
    "    # Visualize improvement\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Before vs After comparison\n",
    "    x = np.arange(len(improvement_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, improvement_df['Original_R2'], width,\n",
    "               label='Original', color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    axes[0].bar(x + width/2, improvement_df['Tuned_R2'], width,\n",
    "               label='Optuna Tuned', color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    axes[0].set_xlabel('Model', fontsize=12)\n",
    "    axes[0].set_ylabel('Test R²', fontsize=12)\n",
    "    axes[0].set_title('Original vs Optuna Tuned Models Performance', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(improvement_df['Model'], rotation=45, ha='right')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add improvement percentages\n",
    "    for i, row in improvement_df.iterrows():\n",
    "        improvement_pct = row['Improvement_Pct']\n",
    "        y_pos = max(row['Original_R2'], row['Tuned_R2']) + 0.01\n",
    "        axes[0].text(i, y_pos, f'+{improvement_pct:.2f}%',\n",
    "                    ha='center', fontweight='bold', fontsize=10, color='green')\n",
    "    \n",
    "    # Improvement magnitude\n",
    "    axes[1].barh(range(len(improvement_df)), improvement_df['Improvement_Pct'],\n",
    "                color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(improvement_df)))\n",
    "    axes[1].set_yticklabels(improvement_df['Model'])\n",
    "    axes[1].set_xlabel('Improvement (%)', fontsize=12)\n",
    "    axes[1].set_title('R² Improvement from Optuna Tuning', fontweight='bold', fontsize=14)\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, val in enumerate(improvement_df['Improvement_Pct']):\n",
    "        axes[1].text(val + 0.1, i, f'{val:.2f}%', va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n Tuning Summary:\")\n",
    "    print(f\"   Average improvement: {improvement_df['Improvement_Pct'].mean():.2f}%\")\n",
    "    print(f\"   Best improvement: {improvement_df['Improvement_Pct'].max():.2f}% ({improvement_df.loc[improvement_df['Improvement_Pct'].idxmax(), 'Model']})\")\n",
    "    print(f\"   Total models tuned: {len(improvement_df)}\")\n",
    "\n",
    "# Select final best model\n",
    "best_final_idx = final_comparison['Test_R2'].idxmax()\n",
    "best_final_model_name = final_comparison.loc[best_final_idx, 'Model']\n",
    "best_final_r2 = final_comparison.loc[best_final_idx, 'Test_R2']\n",
    "best_final_rmse = final_comparison.loc[best_final_idx, 'Test_RMSE']\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\" FINAL BEST MODEL: {best_final_model_name}\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"   Test R²: {best_final_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {best_final_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {final_comparison.loc[best_final_idx, 'Test_MAE']:.4f}\")\n",
    "print(f\"   Test MAPE: {final_comparison.loc[best_final_idx, 'Test_MAPE']:.2f}%\")\n",
    "print(f\"   Overfitting Gap: {final_comparison.loc[best_final_idx, 'Overfit_Gap']:.4f}\")\n",
    "\n",
    "# Get the actual best model object\n",
    "best_model = None\n",
    "\n",
    "# Check if it's a tuned model\n",
    "if best_final_model_name in tuned_models:\n",
    "    best_model = tuned_models[best_final_model_name]\n",
    "    print(f\"\\n Best model retrieved from tuned models\")\n",
    "else:\n",
    "    # It's an original model\n",
    "    if 'Random Forest' in best_final_model_name:\n",
    "        best_model = rf_model\n",
    "    elif 'Gradient Boosting' in best_final_model_name:\n",
    "        best_model = gb_model\n",
    "    elif 'XGBoost' in best_final_model_name:\n",
    "        best_model = xgb_model\n",
    "    elif 'LightGBM' in best_final_model_name:\n",
    "        best_model = lgbm_model\n",
    "    elif 'Linear Regression' in best_final_model_name:\n",
    "        best_model = lr_model\n",
    "    elif 'Ridge' in best_final_model_name:\n",
    "        best_model = ridge_model\n",
    "    elif 'Lasso' in best_final_model_name:\n",
    "        best_model = lasso_model\n",
    "    elif 'ElasticNet' in best_final_model_name:\n",
    "        best_model = elasticnet_model\n",
    "    \n",
    "    if best_model is not None:\n",
    "        print(f\" Best model retrieved from original models\")\n",
    "\n",
    "if best_model is None:\n",
    "    print(f\" Warning: Could not retrieve best model object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CROSS-VALIDATION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Performing 5-Fold Cross-Validation on final best model...\")\n",
    "print(f\"   Model: {best_final_model_name}\")\n",
    "\n",
    "# Determine if we should use scaled or unscaled features\n",
    "use_scaled = any(keyword in best_final_model_name.lower() \n",
    "                for keyword in ['linear', 'ridge', 'lasso', 'elastic'])\n",
    "\n",
    "if use_scaled:\n",
    "    X_cv = X_train_scaled\n",
    "    print(f\"   Using scaled features (linear model)\")\n",
    "else:\n",
    "    X_cv = X_train\n",
    "    print(f\"   Using unscaled features (tree-based model)\")\n",
    "\n",
    "if best_model is not None:\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Calculate multiple metrics\n",
    "    scoring = {\n",
    "        'r2': 'r2',\n",
    "        'neg_rmse': 'neg_root_mean_squared_error',\n",
    "        'neg_mae': 'neg_mean_absolute_error'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n   Running cross-validation...\")\n",
    "    cv_results = cross_validate(\n",
    "        best_model,\n",
    "        X_cv,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Extract scores\n",
    "    train_r2_scores = cv_results['train_r2']\n",
    "    test_r2_scores = cv_results['test_r2']\n",
    "    train_rmse_scores = -cv_results['train_neg_rmse']\n",
    "    test_rmse_scores = -cv_results['test_neg_rmse']\n",
    "    train_mae_scores = -cv_results['train_neg_mae']\n",
    "    test_mae_scores = -cv_results['test_neg_mae']\n",
    "    \n",
    "    print(f\"\\n Cross-Validation Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n R² Scores across 5 folds:\")\n",
    "    print(f\"   {'Fold':<10} {'Train R²':<15} {'Test R²':<15} {'Gap':<10}\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    for i in range(5):\n",
    "        gap = train_r2_scores[i] - test_r2_scores[i]\n",
    "        print(f\"   Fold {i+1:<5} {train_r2_scores[i]:<15.4f} {test_r2_scores[i]:<15.4f} {gap:<10.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Mean Train R²: {train_r2_scores.mean():.4f} (+/- {train_r2_scores.std() * 2:.4f})\")\n",
    "    print(f\"   Mean Test R²:  {test_r2_scores.mean():.4f} (+/- {test_r2_scores.std() * 2:.4f})\")\n",
    "    print(f\"   Mean Gap:      {(train_r2_scores.mean() - test_r2_scores.mean()):.4f}\")\n",
    "    \n",
    "    print(f\"\\nRMSE Scores across 5 folds:\")\n",
    "    print(f\"   {'Fold':<10} {'Train RMSE':<15} {'Test RMSE':<15}\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    for i in range(5):\n",
    "        print(f\"   Fold {i+1:<5} {train_rmse_scores[i]:<15.4f} {test_rmse_scores[i]:<15.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Mean Train RMSE: {train_rmse_scores.mean():.4f} (+/- {train_rmse_scores.std() * 2:.4f})\")\n",
    "    print(f\"   Mean Test RMSE:  {test_rmse_scores.mean():.4f} (+/- {test_rmse_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    print(f\"\\n MAE Scores across 5 folds:\")\n",
    "    print(f\"   {'Fold':<10} {'Train MAE':<15} {'Test MAE':<15}\")\n",
    "    print(f\"   {'-'*50}\")\n",
    "    for i in range(5):\n",
    "        print(f\"   Fold {i+1:<5} {train_mae_scores[i]:<15.4f} {test_mae_scores[i]:<15.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Mean Train MAE: {train_mae_scores.mean():.4f} (+/- {train_mae_scores.std() * 2:.4f})\")\n",
    "    print(f\"   Mean Test MAE:  {test_mae_scores.mean():.4f} (+/- {test_mae_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Visualize CV results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. R² Train vs Test\n",
    "    fold_numbers = np.arange(1, 6)\n",
    "    axes[0, 0].plot(fold_numbers, train_r2_scores, 'o-', label='Train R²', \n",
    "                   linewidth=2, markersize=8, color='blue')\n",
    "    axes[0, 0].plot(fold_numbers, test_r2_scores, 's-', label='Test R²', \n",
    "                   linewidth=2, markersize=8, color='orange')\n",
    "    axes[0, 0].axhline(y=train_r2_scores.mean(), color='blue', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].axhline(y=test_r2_scores.mean(), color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('Fold Number', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('R² Score', fontsize=11)\n",
    "    axes[0, 0].set_title('R² Scores Across Folds', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    axes[0, 0].set_xticks(fold_numbers)\n",
    "    \n",
    "    # 2. R² Box plot\n",
    "    axes[0, 1].boxplot([train_r2_scores, test_r2_scores], \n",
    "                      labels=['Train', 'Test'],\n",
    "                      patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "    axes[0, 1].scatter([1]*5, train_r2_scores, alpha=0.6, s=100, color='blue', zorder=3)\n",
    "    axes[0, 1].scatter([2]*5, test_r2_scores, alpha=0.6, s=100, color='orange', zorder=3)\n",
    "    axes[0, 1].set_ylabel('R² Score', fontsize=11)\n",
    "    axes[0, 1].set_title('R² Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. RMSE Train vs Test\n",
    "    axes[0, 2].plot(fold_numbers, train_rmse_scores, 'o-', label='Train RMSE', \n",
    "                   linewidth=2, markersize=8, color='green')\n",
    "    axes[0, 2].plot(fold_numbers, test_rmse_scores, 's-', label='Test RMSE', \n",
    "                   linewidth=2, markersize=8, color='red')\n",
    "    axes[0, 2].axhline(y=train_rmse_scores.mean(), color='green', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].axhline(y=test_rmse_scores.mean(), color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].set_xlabel('Fold Number', fontsize=11)\n",
    "    axes[0, 2].set_ylabel('RMSE', fontsize=11)\n",
    "    axes[0, 2].set_title('RMSE Across Folds', fontweight='bold', fontsize=12)\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(alpha=0.3)\n",
    "    axes[0, 2].set_xticks(fold_numbers)\n",
    "    \n",
    "    # 4. RMSE Box plot\n",
    "    axes[1, 0].boxplot([train_rmse_scores, test_rmse_scores], \n",
    "                      labels=['Train', 'Test'],\n",
    "                      patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightcoral', alpha=0.7))\n",
    "    axes[1, 0].scatter([1]*5, train_rmse_scores, alpha=0.6, s=100, color='green', zorder=3)\n",
    "    axes[1, 0].scatter([2]*5, test_rmse_scores, alpha=0.6, s=100, color='red', zorder=3)\n",
    "    axes[1, 0].set_ylabel('RMSE', fontsize=11)\n",
    "    axes[1, 0].set_title('RMSE Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. MAE Train vs Test\n",
    "    axes[1, 1].plot(fold_numbers, train_mae_scores, 'o-', label='Train MAE', \n",
    "                   linewidth=2, markersize=8, color='purple')\n",
    "    axes[1, 1].plot(fold_numbers, test_mae_scores, 's-', label='Test MAE', \n",
    "                   linewidth=2, markersize=8, color='brown')\n",
    "    axes[1, 1].axhline(y=train_mae_scores.mean(), color='purple', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].axhline(y=test_mae_scores.mean(), color='brown', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].set_xlabel('Fold Number', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('MAE', fontsize=11)\n",
    "    axes[1, 1].set_title('MAE Across Folds', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    axes[1, 1].set_xticks(fold_numbers)\n",
    "    \n",
    "    # 6. MAE Box plot\n",
    "    axes[1, 2].boxplot([train_mae_scores, test_mae_scores], \n",
    "                      labels=['Train', 'Test'],\n",
    "                      patch_artist=True,\n",
    "                      boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
    "    axes[1, 2].scatter([1]*5, train_mae_scores, alpha=0.6, s=100, color='purple', zorder=3)\n",
    "    axes[1, 2].scatter([2]*5, test_mae_scores, alpha=0.6, s=100, color='brown', zorder=3)\n",
    "    axes[1, 2].set_ylabel('MAE', fontsize=11)\n",
    "    axes[1, 2].set_title('MAE Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[1, 2].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle(f'Cross-Validation Analysis - {best_final_model_name}', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Stability assessment\n",
    "    r2_std = test_r2_scores.std()\n",
    "    rmse_std = test_rmse_scores.std()\n",
    "    \n",
    "    print(f\"\\n Model Stability Assessment:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Test R² std: {r2_std:.4f}\")\n",
    "    print(f\"   Test RMSE std: {rmse_std:.4f}\")\n",
    "    \n",
    "    if r2_std < 0.05:\n",
    "        print(f\"\\n    Model is very stable! (R² std < 0.05)\")\n",
    "    elif r2_std < 0.1:\n",
    "        print(f\"\\n    Model is reasonably stable (R² std < 0.1)\")\n",
    "    else:\n",
    "        print(f\"\\n    Model shows high variance across folds (R² std >= 0.1)\")\n",
    "        print(f\"      Consider collecting more data or adjusting regularization\")\n",
    "    \n",
    "    # Overfitting check\n",
    "    avg_gap = (train_r2_scores.mean() - test_r2_scores.mean())\n",
    "    print(f\"\\n Overfitting Check:\")\n",
    "    print(f\"   Average Train-Test Gap: {avg_gap:.4f}\")\n",
    "    \n",
    "    if avg_gap < 0.05:\n",
    "        print(f\"   Excellent! Model generalizes very well\")\n",
    "    elif avg_gap < 0.1:\n",
    "        print(f\"    Good! Model generalizes well\")\n",
    "    elif avg_gap < 0.15:\n",
    "        print(f\"    Slight overfitting detected\")\n",
    "    else:\n",
    "        print(f\"    Significant overfitting detected - consider regularization\")\n",
    "    \n",
    "    print(f\"\\n Cross-validation analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n Could not find best model object for CV analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESIDUAL ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESIDUAL ANALYSIS - BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Analyzing residuals for: {best_final_model_name}\")\n",
    "\n",
    "# Get predictions from best model\n",
    "best_result = None\n",
    "for result in all_models_results:\n",
    "    if result['Model'] == best_final_model_name:\n",
    "        best_result = result\n",
    "        break\n",
    "\n",
    "if best_result is not None:\n",
    "    y_train_pred = best_result['Predictions_Train']\n",
    "    y_test_pred = best_result['Predictions_Test']\n",
    "    \n",
    "    # Calculate residuals\n",
    "    train_residuals = y_train - y_train_pred\n",
    "    test_residuals = y_test - y_test_pred\n",
    "    \n",
    "    print(f\"\\n Residual Statistics:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training Residuals:\")\n",
    "    print(f\"   Mean: {train_residuals.mean():.4f}\")\n",
    "    print(f\"   Std: {train_residuals.std():.4f}\")\n",
    "    print(f\"   Min: {train_residuals.min():.4f}\")\n",
    "    print(f\"   Max: {train_residuals.max():.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Residuals:\")\n",
    "    print(f\"   Mean: {test_residuals.mean():.4f}\")\n",
    "    print(f\"   Std: {test_residuals.std():.4f}\")\n",
    "    print(f\"   Min: {test_residuals.min():.4f}\")\n",
    "    print(f\"   Max: {test_residuals.max():.4f}\")\n",
    "    \n",
    "    # Create comprehensive residual plots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Residuals vs Predicted (Train)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.scatter(y_train_pred, train_residuals, alpha=0.5, s=20, color='blue')\n",
    "    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax1.set_xlabel('Predicted Values')\n",
    "    ax1.set_ylabel('Residuals')\n",
    "    ax1.set_title('Train: Residuals vs Predicted', fontweight='bold')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Residuals vs Predicted (Test)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.scatter(y_test_pred, test_residuals, alpha=0.5, s=20, color='orange')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Predicted Values')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.set_title('Test: Residuals vs Predicted', fontweight='bold')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Distribution of Residuals (Train)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.hist(train_residuals, bins=50, edgecolor='black', alpha=0.7, color='blue')\n",
    "    ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax3.set_xlabel('Residuals')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Train: Residual Distribution', fontweight='bold')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Distribution of Residuals (Test)\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.hist(test_residuals, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax4.set_xlabel('Residuals')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Test: Residual Distribution', fontweight='bold')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Q-Q Plot (Train)\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    stats.probplot(train_residuals, dist=\"norm\", plot=ax5)\n",
    "    ax5.set_title('Train: Q-Q Plot', fontweight='bold')\n",
    "    ax5.grid(alpha=0.3)\n",
    "    \n",
    "    # 6. Q-Q Plot (Test)\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    stats.probplot(test_residuals, dist=\"norm\", plot=ax6)\n",
    "    ax6.set_title('Test: Q-Q Plot', fontweight='bold')\n",
    "    ax6.grid(alpha=0.3)\n",
    "    \n",
    "    # 7. Actual vs Predicted (Train)\n",
    "    ax7 = fig.add_subplot(gs[2, 0])\n",
    "    ax7.scatter(y_train, y_train_pred, alpha=0.5, s=20, color='blue')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_train.min(), y_train_pred.min())\n",
    "    max_val = max(y_train.max(), y_train_pred.max())\n",
    "    ax7.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax7.set_xlabel('Actual Values')\n",
    "    ax7.set_ylabel('Predicted Values')\n",
    "    ax7.set_title('Train: Actual vs Predicted', fontweight='bold')\n",
    "    ax7.legend()\n",
    "    ax7.grid(alpha=0.3)\n",
    "    \n",
    "    # 8. Actual vs Predicted (Test)\n",
    "    ax8 = fig.add_subplot(gs[2, 1])\n",
    "    ax8.scatter(y_test, y_test_pred, alpha=0.5, s=20, color='orange')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_test_pred.min())\n",
    "    max_val = max(y_test.max(), y_test_pred.max())\n",
    "    ax8.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax8.set_xlabel('Actual Values')\n",
    "    ax8.set_ylabel('Predicted Values')\n",
    "    ax8.set_title('Test: Actual vs Predicted', fontweight='bold')\n",
    "    ax8.legend()\n",
    "    ax8.grid(alpha=0.3)\n",
    "    \n",
    "    # 9. Residual Box Plot Comparison\n",
    "    ax9 = fig.add_subplot(gs[2, 2])\n",
    "    ax9.boxplot([train_residuals, test_residuals], \n",
    "                labels=['Train', 'Test'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "    ax9.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax9.set_ylabel('Residuals')\n",
    "    ax9.set_title('Residuals Comparison', fontweight='bold')\n",
    "    ax9.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Residual Analysis - {best_final_model_name}', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(f\"\\n Statistical Tests:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Shapiro-Wilk test for normality (sample if too large)\n",
    "    sample_size = min(5000, len(test_residuals))\n",
    "    test_residuals_sample = np.random.choice(test_residuals, size=sample_size, replace=False)\n",
    "    \n",
    "    shapiro_stat, shapiro_p = stats.shapiro(test_residuals_sample)\n",
    "    print(f\"Shapiro-Wilk Test (Normality of Residuals):\")\n",
    "    print(f\"   Statistic: {shapiro_stat:.4f}\")\n",
    "    print(f\"   P-value: {shapiro_p:.4f}\")\n",
    "    \n",
    "    if shapiro_p > 0.05:\n",
    "        print(f\"    Residuals are approximately normally distributed (p > 0.05)\")\n",
    "    else:\n",
    "        print(f\"    Residuals may not be normally distributed (p < 0.05)\")\n",
    "    \n",
    "    # Check for patterns (randomness)\n",
    "    print(f\"\\n Residual Pattern Check:\")\n",
    "    \n",
    "    # Calculate correlation between predictions and residuals\n",
    "    corr_train = np.corrcoef(y_train_pred, train_residuals)[0, 1]\n",
    "    corr_test = np.corrcoef(y_test_pred, test_residuals)[0, 1]\n",
    "    \n",
    "    print(f\"   Correlation (Predictions vs Residuals):\")\n",
    "    print(f\"      Train: {corr_train:.4f}\")\n",
    "    print(f\"      Test: {corr_test:.4f}\")\n",
    "    \n",
    "    if abs(corr_test) < 0.1:\n",
    "        print(f\"    Residuals show no pattern (good!)\")\n",
    "    else:\n",
    "        print(f\"    Residuals show some pattern (model may miss non-linear relationships)\")\n",
    "    \n",
    "    print(f\"\\n Residual analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n Could not find best model results for residual analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICTION VISUALIZATION (ORIGINAL SCALE)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION VISUALIZATION - ORIGINAL SCALE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Converting log-transformed predictions back to original PM2.5 scale...\")\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "# Remember: we used log1p, so we use expm1 to reverse\n",
    "y_train_pred_original = np.expm1(y_train_pred)\n",
    "y_test_pred_original = np.expm1(y_test_pred)\n",
    "\n",
    "# Calculate metrics on original scale\n",
    "train_rmse_original = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))\n",
    "test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))\n",
    "\n",
    "train_mae_original = mean_absolute_error(y_train_original, y_train_pred_original)\n",
    "test_mae_original = mean_absolute_error(y_test_original, y_test_pred_original)\n",
    "\n",
    "train_r2_original = r2_score(y_train_original, y_train_pred_original)\n",
    "test_r2_original = r2_score(y_test_original, y_test_pred_original)\n",
    "\n",
    "train_mape_original = np.mean(np.abs((y_train_original - y_train_pred_original) / (y_train_original + 1e-10))) * 100\n",
    "test_mape_original = np.mean(np.abs((y_test_original - y_test_pred_original) / (y_test_original + 1e-10))) * 100\n",
    "\n",
    "print(f\"\\n Performance on Original PM2.5 Scale (μg/m³):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training Set:\")\n",
    "print(f\"   RMSE: {train_rmse_original:.2f} μg/m³\")\n",
    "print(f\"   MAE: {train_mae_original:.2f} μg/m³\")\n",
    "print(f\"   R²: {train_r2_original:.4f}\")\n",
    "print(f\"   MAPE: {train_mape_original:.2f}%\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"   RMSE: {test_rmse_original:.2f} μg/m³\")\n",
    "print(f\"   MAE: {test_mae_original:.2f} μg/m³\")\n",
    "print(f\"   R²: {test_r2_original:.4f}\")\n",
    "print(f\"   MAPE: {test_mape_original:.2f}%\")\n",
    "\n",
    "# Visualizations on original scale\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Actual vs Predicted - Train\n",
    "axes[0, 0].scatter(y_train_original, y_train_pred_original, alpha=0.5, s=20, color='blue')\n",
    "min_val = min(y_train_original.min(), y_train_pred_original.min())\n",
    "max_val = max(y_train_original.max(), y_train_pred_original.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual PM2.5 (μg/m³)')\n",
    "axes[0, 0].set_ylabel('Predicted PM2.5 (μg/m³)')\n",
    "axes[0, 0].set_title(f'Train: Actual vs Predicted\\nR² = {train_r2_original:.4f}', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted - Test\n",
    "axes[0, 1].scatter(y_test_original, y_test_pred_original, alpha=0.5, s=20, color='orange')\n",
    "min_val = min(y_test_original.min(), y_test_pred_original.min())\n",
    "max_val = max(y_test_original.max(), y_test_pred_original.max())\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual PM2.5 (μg/m³)')\n",
    "axes[0, 1].set_ylabel('Predicted PM2.5 (μg/m³)')\n",
    "axes[0, 1].set_title(f'Test: Actual vs Predicted\\nR² = {test_r2_original:.4f}', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Prediction Error Distribution - Train\n",
    "train_errors = y_train_original - y_train_pred_original\n",
    "axes[0, 2].hist(train_errors, bins=50, edgecolor='black', alpha=0.7, color='blue')\n",
    "axes[0, 2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 2].set_xlabel('Prediction Error (μg/m³)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title(f'Train: Error Distribution\\nMAE = {train_mae_original:.2f} μg/m³', fontweight='bold')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# 4. Prediction Error Distribution - Test\n",
    "test_errors = y_test_original - y_test_pred_original\n",
    "axes[1, 0].hist(test_errors, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Prediction Error (μg/m³)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Test: Error Distribution\\nMAE = {test_mae_original:.2f} μg/m³', fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 5. Sample Predictions Comparison - Test (first 100)\n",
    "sample_size = min(100, len(y_test_original))\n",
    "sample_idx = np.arange(sample_size)\n",
    "\n",
    "axes[1, 1].plot(sample_idx, y_test_original[:sample_size], 'o-', label='Actual', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[1, 1].plot(sample_idx, y_test_pred_original[:sample_size], 's-', label='Predicted', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[1, 1].set_xlabel('Sample Index')\n",
    "axes[1, 1].set_ylabel('PM2.5 (μg/m³)')\n",
    "axes[1, 1].set_title('Test: Sample Predictions (First 100)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# 6. Percentage Error Distribution - Test\n",
    "percentage_errors = ((y_test_original - y_test_pred_original) / (y_test_original + 1e-10)) * 100\n",
    "axes[1, 2].hist(percentage_errors, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1, 2].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 2].set_xlabel('Percentage Error (%)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title(f'Test: Percentage Error\\nMAPE = {test_mape_original:.2f}%', fontweight='bold')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Prediction Analysis - {best_final_model_name} (Original PM2.5 Scale)', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction quality by PM2.5 ranges\n",
    "print(f\"\\n Prediction Quality by PM2.5 Ranges:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Define PM2.5 ranges (EPA Air Quality Index)\n",
    "ranges = [\n",
    "    (0, 12, 'Good'),\n",
    "    (12, 35, 'Moderate'),\n",
    "    (35, 55, 'Unhealthy for Sensitive'),\n",
    "    (55, 150, 'Unhealthy'),\n",
    "    (150, 250, 'Very Unhealthy'),\n",
    "    (250, 1000, 'Hazardous')\n",
    "]\n",
    "\n",
    "for low, high, label in ranges:\n",
    "    mask = (y_test_original >= low) & (y_test_original < high)\n",
    "    count = mask.sum()\n",
    "    \n",
    "    if count > 0:\n",
    "        range_mae = mean_absolute_error(y_test_original[mask], y_test_pred_original[mask])\n",
    "        range_mape = np.mean(np.abs((y_test_original[mask] - y_test_pred_original[mask]) / (y_test_original[mask] + 1e-10))) * 100\n",
    "        \n",
    "        print(f\"\\n{label} ({low}-{high} μg/m³):\")\n",
    "        print(f\"   Samples: {count}\")\n",
    "        print(f\"   MAE: {range_mae:.2f} μg/m³\")\n",
    "        print(f\"   MAPE: {range_mape:.2f}%\")\n",
    "\n",
    "print(f\"\\n Prediction visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE - BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Analyzing feature importance for: {best_final_model_name}\")\n",
    "\n",
    "# Get feature importance from best model\n",
    "if best_model is not None and hasattr(best_model, 'feature_importances_'):\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n Top 30 Most Important Features:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for i, row in enumerate(feature_importance.head(30).itertuples(), 1):\n",
    "        print(f\"   {i:2d}. {row.Feature[:55]:<55} {row.Importance:.6f}\")\n",
    "    \n",
    "\n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "    \n",
    "    # 1. Top 20 features\n",
    "    top_20 = feature_importance.head(20)\n",
    "    axes[0, 0].barh(range(len(top_20)), top_20['Importance'], color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_yticks(range(len(top_20)))\n",
    "    axes[0, 0].set_yticklabels([f[:40] + '...' if len(f) > 40 else f for f in top_20['Feature']], fontsize=9)\n",
    "    axes[0, 0].set_xlabel('Importance', fontsize=11)\n",
    "    axes[0, 0].set_title('Top 20 Most Important Features', fontweight='bold', fontsize=13)\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Cumulative importance\n",
    "    cumulative_importance = feature_importance['Importance'].cumsum() / feature_importance['Importance'].sum()\n",
    "    axes[0, 1].plot(range(1, len(cumulative_importance) + 1), cumulative_importance, linewidth=2, color='darkgreen')\n",
    "    axes[0, 1].axhline(y=0.8, color='red', linestyle='--', linewidth=2, label='80% threshold')\n",
    "    axes[0, 1].axhline(y=0.9, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
    "    axes[0, 1].set_xlabel('Number of Features', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Cumulative Importance', fontsize=11)\n",
    "    axes[0, 1].set_title('Cumulative Feature Importance', fontweight='bold', fontsize=13)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Find features needed for 80% and 90%\n",
    "    features_80 = (cumulative_importance >= 0.8).argmax() + 1\n",
    "    features_90 = (cumulative_importance >= 0.9).argmax() + 1\n",
    "    \n",
    "    axes[0, 1].axvline(x=features_80, color='red', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    axes[0, 1].axvline(x=features_90, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    print(f\"\\n Feature Importance Statistics:\")\n",
    "    print(f\"   Features for 80% importance: {features_80}/{len(feature_importance)}\")\n",
    "    print(f\"   Features for 90% importance: {features_90}/{len(feature_importance)}\")\n",
    "    \n",
    "    # 3. Feature categories importance\n",
    "    feature_categories_importance = {\n",
    "        'Time Series': 0,\n",
    "        'Temporal': 0,\n",
    "        'Weather': 0,\n",
    "        'Pollutants': 0,\n",
    "        'Location': 0,\n",
    "        'Wind': 0,\n",
    "        'Interactions': 0,\n",
    "        'Other': 0\n",
    "    }\n",
    "    \n",
    "    for _, row in feature_importance.iterrows():\n",
    "        feature = row['Feature']\n",
    "        importance = row['Importance']\n",
    "        \n",
    "        if 'rolling' in feature or 'lag' in feature or 'diff' in feature:\n",
    "            feature_categories_importance['Time Series'] += importance\n",
    "        elif 'month' in feature or 'day' in feature or 'season' in feature:\n",
    "            feature_categories_importance['Temporal'] += importance\n",
    "        elif 'temperature' in feature or 'humidity' in feature or 'precipitable' in feature:\n",
    "            feature_categories_importance['Weather'] += importance\n",
    "        elif 'L3_' in feature and any(x in feature for x in ['NO2', 'CO', 'SO2', 'HCHO', 'O3']):\n",
    "            feature_categories_importance['Pollutants'] += importance\n",
    "        elif 'place_' in feature:\n",
    "            feature_categories_importance['Location'] += importance\n",
    "        elif 'wind' in feature.lower():\n",
    "            feature_categories_importance['Wind'] += importance\n",
    "        elif 'interaction' in feature or 'pollutant_load' in feature or 'AQI' in feature or 'ratio' in feature:\n",
    "            feature_categories_importance['Interactions'] += importance\n",
    "        else:\n",
    "            feature_categories_importance['Other'] += importance\n",
    "    \n",
    "    # Sort categories\n",
    "    categories_sorted = dict(sorted(feature_categories_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    axes[1, 0].bar(range(len(categories_sorted)), list(categories_sorted.values()),\n",
    "                  color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xticks(range(len(categories_sorted)))\n",
    "    axes[1, 0].set_xticklabels(list(categories_sorted.keys()), rotation=45, ha='right')\n",
    "    axes[1, 0].set_ylabel('Total Importance', fontsize=11)\n",
    "    axes[1, 0].set_title('Feature Category Importance', fontweight='bold', fontsize=13)\n",
    "    axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, (cat, val) in enumerate(categories_sorted.items()):\n",
    "        axes[1, 0].text(i, val + 0.01, f'{val:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    print(f\"\\n Feature Category Importance:\")\n",
    "    for category, importance in categories_sorted.items():\n",
    "        print(f\"   {category:<20}: {importance:.4f}\")\n",
    "    \n",
    "    # 4. Top features breakdown\n",
    "    top_10 = feature_importance.head(10)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_10)))\n",
    "    \n",
    "    axes[1, 1].barh(range(len(top_10)), top_10['Importance'], color=colors, edgecolor='black', alpha=0.8)\n",
    "    axes[1, 1].set_yticks(range(len(top_10)))\n",
    "    axes[1, 1].set_yticklabels([f[:35] + '...' if len(f) > 35 else f for f in top_10['Feature']], fontsize=10)\n",
    "    axes[1, 1].set_xlabel('Importance', fontsize=11)\n",
    "    axes[1, 1].set_title('Top 10 Features (Detailed)', fontweight='bold', fontsize=13)\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total_importance = feature_importance['Importance'].sum()\n",
    "    for i, (idx, row) in enumerate(top_10.iterrows()):\n",
    "        pct = (row['Importance'] / total_importance) * 100\n",
    "        axes[1, 1].text(row['Importance'] + 0.002, i, f'{pct:.1f}%', \n",
    "                       va='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.suptitle(f'Feature Importance Analysis - {best_final_model_name}', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Feature importance analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n Best model does not have feature_importances_ attribute\")\n",
    "    print(f\"   (Linear models don't provide feature importance in the same way)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE BEST MODEL AND ARTIFACTS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING BEST MODEL AND ARTIFACTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../artifacts/models', exist_ok=True)\n",
    "os.makedirs('../artifacts/scalers', exist_ok=True)\n",
    "os.makedirs('../artifacts/reports', exist_ok=True)\n",
    "\n",
    "print(f\"\\n Saving model artifacts...\")\n",
    "\n",
    "# 1. Save the best model\n",
    "if best_model is not None:\n",
    "    model_filename = f'../artifacts/models/best_model_{best_final_model_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}.pkl'\n",
    "    \n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    print(f\"   Best model saved: {model_filename}\")\n",
    "    print(f\"      Model: {best_final_model_name}\")\n",
    "else:\n",
    "    print(f\"   Warning: Best model object not found, could not save model\")\n",
    "\n",
    "# 2. Save the scaler (for preprocessing)\n",
    "scaler_filename = '../artifacts/scalers/standard_scaler.pkl'\n",
    "\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"    Scaler saved: {scaler_filename}\")\n",
    "\n",
    "# 3. Save feature names\n",
    "feature_names_file = '../artifacts/models/feature_names.txt'\n",
    "\n",
    "with open(feature_names_file, 'w') as f:\n",
    "    for feature in X_train.columns:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"    Feature names saved: {feature_names_file}\")\n",
    "\n",
    "# 4. Save model comparison results\n",
    "comparison_file = '../artifacts/reports/model_comparison.csv'\n",
    "final_comparison.to_csv(comparison_file, index=False)\n",
    "\n",
    "print(f\"    Model comparison saved: {comparison_file}\")\n",
    "\n",
    "# 5. Save predictions\n",
    "predictions_train_file = '../artifacts/reports/predictions_train.csv'\n",
    "predictions_test_file = '../artifacts/reports/predictions_test.csv'\n",
    "\n",
    "# Create prediction dataframes\n",
    "if best_result is not None:\n",
    "    # Training predictions\n",
    "    predictions_train_df = pd.DataFrame({\n",
    "        'Actual_Log': y_train,\n",
    "        'Predicted_Log': y_train_pred,\n",
    "        'Actual_Original': y_train_original,\n",
    "        'Predicted_Original': y_train_pred_original,\n",
    "        'Error': y_train_original - y_train_pred_original,\n",
    "        'Absolute_Error': np.abs(y_train_original - y_train_pred_original),\n",
    "        'Percentage_Error': ((y_train_original - y_train_pred_original) / (y_train_original + 1e-10)) * 100\n",
    "    })\n",
    "    \n",
    "    # Add metadata if available\n",
    "    if len(metadata_train) == len(predictions_train_df):\n",
    "        predictions_train_df = pd.concat([metadata_train.reset_index(drop=True), predictions_train_df], axis=1)\n",
    "    \n",
    "    predictions_train_df.to_csv(predictions_train_file, index=False)\n",
    "    print(f\"    Training predictions saved: {predictions_train_file}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    predictions_test_df = pd.DataFrame({\n",
    "        'Actual_Log': y_test,\n",
    "        'Predicted_Log': y_test_pred,\n",
    "        'Actual_Original': y_test_original,\n",
    "        'Predicted_Original': y_test_pred_original,\n",
    "        'Error': y_test_original - y_test_pred_original,\n",
    "        'Absolute_Error': np.abs(y_test_original - y_test_pred_original),\n",
    "        'Percentage_Error': ((y_test_original - y_test_pred_original) / (y_test_original + 1e-10)) * 100\n",
    "    })\n",
    "    \n",
    "    # Add metadata if available\n",
    "    if len(metadata_test) == len(predictions_test_df):\n",
    "        predictions_test_df = pd.concat([metadata_test.reset_index(drop=True), predictions_test_df], axis=1)\n",
    "    \n",
    "    predictions_test_df.to_csv(predictions_test_file, index=False)\n",
    "    print(f\"   Test predictions saved: {predictions_test_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXTRACT FEATURE MEDIANS FOR DEPLOYMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING FEATURE MEDIANS FOR DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../artifacts/feature_engineering', exist_ok=True)\n",
    "\n",
    "print(f\"\\n Analyzing training features...\")\n",
    "print(f\"   Total features: {len(X_train.columns)}\")\n",
    "\n",
    "# ============================================\n",
    "# IDENTIFY UNCALCULABLE FEATURES\n",
    "# ============================================\n",
    "\n",
    "# Features that CANNOT be calculated from single raw input\n",
    "time_series_features = [col for col in X_train.columns if any(x in col for x in ['rolling', 'lag', 'diff', 'ewm', 'shift'])]\n",
    "location_features = [col for col in X_train.columns if 'place_' in col and 'place_id' not in col.lower()]\n",
    "pca_features = [col for col in X_train.columns if 'pca' in col.lower()]\n",
    "cluster_features = [col for col in X_train.columns if 'cluster' in col]\n",
    "historical_agg_features = [col for col in X_train.columns if any(x in col for x in ['_mean_', '_median_', '_std_', '_min_', '_max_'])]\n",
    "\n",
    "# Combine all uncalculable features\n",
    "uncalculable_features = list(set(\n",
    "    time_series_features + \n",
    "    location_features + \n",
    "    pca_features + \n",
    "    cluster_features + \n",
    "    historical_agg_features\n",
    "))\n",
    "\n",
    "print(f\"\\n Feature Categories:\")\n",
    "print(f\"   - Time series (rolling, lag, diff): {len(time_series_features)}\")\n",
    "print(f\"   - Location aggregations: {len(location_features)}\")\n",
    "print(f\"   - PCA components: {len(pca_features)}\")\n",
    "print(f\"   - Cluster features: {len(cluster_features)}\")\n",
    "print(f\"   - Historical aggregations: {len(historical_agg_features)}\")\n",
    "print(f\"   - Total UNCALCULABLE: {len(uncalculable_features)}\")\n",
    "print(f\"   - Total CALCULABLE: {len(X_train.columns) - len(uncalculable_features)}\")\n",
    "\n",
    "# ============================================\n",
    "# CALCULATE MEDIANS\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n Calculating medians...\")\n",
    "\n",
    "# Medians for uncalculable features only\n",
    "uncalculable_medians = {}\n",
    "for feature in uncalculable_features:\n",
    "    if feature in X_train.columns:\n",
    "        median_value = float(X_train[feature].median())\n",
    "        uncalculable_medians[feature] = median_value\n",
    "\n",
    "# Medians for ALL features (backup)\n",
    "all_feature_medians = {col: float(X_train[col].median()) for col in X_train.columns}\n",
    "\n",
    "print(f\"    Calculated medians for {len(uncalculable_medians)} uncalculable features\")\n",
    "print(f\"    Calculated medians for {len(all_feature_medians)} total features\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE TO JSON\n",
    "# ============================================\n",
    "\n",
    "medians_dict = {\n",
    "    'uncalculable_features': uncalculable_medians,\n",
    "    'all_features': all_feature_medians,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'metadata': {\n",
    "        'n_features_total': len(X_train.columns),\n",
    "        'n_features_uncalculable': len(uncalculable_features),\n",
    "        'n_features_calculable': len(X_train.columns) - len(uncalculable_features),\n",
    "        'training_samples': len(X_train),\n",
    "        'date_saved': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'best_model': best_final_model_name if 'best_final_model_name' in locals() else 'Unknown'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "medians_file = '../artifacts/feature_engineering/feature_medians.json'\n",
    "\n",
    "with open(medians_file, 'w') as f:\n",
    "    json.dump(medians_dict, f, indent=4)\n",
    "\n",
    "print(f\"\\n Feature medians saved to: {medians_file}\")\n",
    "\n",
    "# ============================================\n",
    "# DISPLAY SAMPLE\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n Sample Uncalculable Feature Medians:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "sample_features = list(uncalculable_medians.items())[:15]\n",
    "for i, (feature, median) in enumerate(sample_features, 1):\n",
    "    feature_display = feature[:50] + '...' if len(feature) > 50 else feature\n",
    "    print(f\"   {i:2d}. {feature_display:<53} {median:>10.4f}\")\n",
    "\n",
    "if len(uncalculable_medians) > 15:\n",
    "    print(f\"   ... and {len(uncalculable_medians) - 15} more features\")\n",
    "\n",
    "print(f\"\\n Feature medians extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXTRACT LOCATION-SPECIFIC FEATURES FOR PRODUCTION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTING LOCATION-SPECIFIC FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Analyzing locations in training data...\")\n",
    "\n",
    "# ============================================\n",
    "# 1. CHECK IF WE HAVE LOCATION DATA\n",
    "# ============================================\n",
    "\n",
    "# Since we don't have the original df, we'll check if there's a location column in X_train\n",
    "location_columns = [col for col in X_train.columns if 'place' in col.lower() and 'id' in col.lower()]\n",
    "\n",
    "if len(location_columns) > 0:\n",
    "    location_col = location_columns[0]\n",
    "    print(f\" Found location column: {location_col}\")\n",
    "    has_locations = True\n",
    "else:\n",
    "    print(\" No location column found in X_train.\")\n",
    "    print(\"   Creating a single 'global' location for all data.\")\n",
    "    has_locations = False\n",
    "    location_col = 'location'\n",
    "\n",
    "# ============================================\n",
    "# 2. IDENTIFY TIME-SERIES FEATURES\n",
    "# ============================================\n",
    "\n",
    "# These are features that depend on historical data\n",
    "time_series_features = [col for col in X_train.columns if any(x in col for x in \n",
    "    ['rolling', 'lag', 'diff', 'ewm', 'shift', '_avg', '_std', \n",
    "     '_min', '_max', '_median']) and col != location_col]\n",
    "\n",
    "print(f\"\\n Time-series features to extract: {len(time_series_features)}\")\n",
    "\n",
    "# ============================================\n",
    "# 3. EXTRACT FEATURES PER LOCATION\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n Extracting features for each location...\")\n",
    "\n",
    "location_features = {}\n",
    "\n",
    "if has_locations:\n",
    "    # We have actual locations\n",
    "    unique_locations = X_train[location_col].unique()\n",
    "    print(f\"   Found {len(unique_locations)} unique locations\")\n",
    "    \n",
    "    for location in unique_locations:\n",
    "        # Get all rows for this location in training data\n",
    "        location_mask = X_train[location_col] == location\n",
    "        location_data = X_train[location_mask]\n",
    "        \n",
    "        if len(location_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate statistics for time-series features\n",
    "        location_stats = {}\n",
    "        \n",
    "        for feature in time_series_features:\n",
    "            if feature in location_data.columns:\n",
    "                # Use the LAST known value (most recent)\n",
    "                last_value = location_data[feature].iloc[-1]\n",
    "                \n",
    "                # Also calculate median (more robust)\n",
    "                median_value = location_data[feature].median()\n",
    "                \n",
    "                # Use last value if available, otherwise median\n",
    "                if pd.notna(last_value):\n",
    "                    location_stats[feature] = float(last_value)\n",
    "                else:\n",
    "                    location_stats[feature] = float(median_value) if pd.notna(median_value) else 0.0\n",
    "        \n",
    "        # Store for this location\n",
    "        location_features[str(location)] = {\n",
    "            'features': location_stats,\n",
    "            'n_samples': int(len(location_data)),\n",
    "            'last_seen': 'unknown'\n",
    "        }\n",
    "    \n",
    "    print(f\" Extracted features for {len(location_features)} locations\")\n",
    "    \n",
    "else:\n",
    "    # No location column - create one global location\n",
    "    print(\"   Creating single global location...\")\n",
    "    \n",
    "    location_stats = {}\n",
    "    \n",
    "    for feature in time_series_features:\n",
    "        if feature in X_train.columns:\n",
    "            # Use last value\n",
    "            last_value = X_train[feature].iloc[-1]\n",
    "            \n",
    "            if pd.notna(last_value):\n",
    "                location_stats[feature] = float(last_value)\n",
    "            else:\n",
    "                median_value = X_train[feature].median()\n",
    "                location_stats[feature] = float(median_value) if pd.notna(median_value) else 0.0\n",
    "    \n",
    "    location_features['global'] = {\n",
    "        'features': location_stats,\n",
    "        'n_samples': len(X_train),\n",
    "        'last_seen': 'unknown'\n",
    "    }\n",
    "    \n",
    "    print(f\" Created global location with {len(X_train)} samples\")\n",
    "\n",
    "# ============================================\n",
    "# 4. CALCULATE GLOBAL FALLBACK (for new locations)\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n Calculating global fallback features...\")\n",
    "\n",
    "global_features = {}\n",
    "\n",
    "for feature in time_series_features:\n",
    "    if feature in X_train.columns:\n",
    "        # Use median across all data\n",
    "        median_value = X_train[feature].median()\n",
    "        global_features[feature] = float(median_value) if pd.notna(median_value) else 0.0\n",
    "\n",
    "print(f\" Calculated {len(global_features)} global fallback features\")\n",
    "\n",
    "# ============================================\n",
    "# 5. CREATE FINAL LOOKUP STRUCTURE\n",
    "# ============================================\n",
    "\n",
    "location_lookup = {\n",
    "    'locations': location_features,\n",
    "    'global_fallback': global_features,\n",
    "    'metadata': {\n",
    "        'n_locations': len(location_features),\n",
    "        'n_time_series_features': len(time_series_features),\n",
    "        'location_column': location_col,\n",
    "        'has_location_data': has_locations,\n",
    "        'total_training_samples': len(X_train),\n",
    "        'date_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'feature_list': time_series_features\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 6. SAVE TO JSON\n",
    "# ============================================\n",
    "\n",
    "lookup_file = '../artifacts/feature_engineering/location_features_lookup.json'\n",
    "\n",
    "with open(lookup_file, 'w') as f:\n",
    "    json.dump(location_lookup, f, indent=4)\n",
    "\n",
    "print(f\"\\n Location features saved to: {lookup_file}\")\n",
    "\n",
    "# ============================================\n",
    "# 7. SAVE LOCATION LIST (for dropdown in UI)\n",
    "# ============================================\n",
    "\n",
    "location_ids = list(location_features.keys())\n",
    "\n",
    "location_list = {\n",
    "    'locations': [\n",
    "        {\n",
    "            'id': str(loc),\n",
    "            'name': str(loc) if loc != 'global' else 'Global (All Data)',\n",
    "            'n_samples': location_features[str(loc)]['n_samples'],\n",
    "            'last_seen': location_features[str(loc)]['last_seen']\n",
    "        }\n",
    "        for loc in location_ids\n",
    "    ],\n",
    "    'metadata': {\n",
    "        'n_locations': len(location_ids),\n",
    "        'default_location': str(location_ids[0]) if len(location_ids) > 0 else 'global',\n",
    "        'has_location_data': has_locations\n",
    "    }\n",
    "}\n",
    "\n",
    "location_list_file = '../artifacts/feature_engineering/available_locations.json'\n",
    "\n",
    "with open(location_list_file, 'w') as f:\n",
    "    json.dump(location_list, f, indent=4)\n",
    "\n",
    "print(f\" Available locations saved to: {location_list_file}\")\n",
    "\n",
    "# ============================================\n",
    "# 8. DISPLAY SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\" LOCATION FEATURES EXTRACTION SUMMARY\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "print(f\"\\n Files Created:\")\n",
    "print(f\"   1. {lookup_file}\")\n",
    "print(f\"      - Features for {len(location_features)} location(s)\")\n",
    "print(f\"      - {len(time_series_features)} time-series features per location\")\n",
    "print(f\"   2. {location_list_file}\")\n",
    "print(f\"      - List of available locations for UI dropdown\")\n",
    "\n",
    "if has_locations:\n",
    "    print(f\"\\n Sample Locations:\")\n",
    "    sample_locations = list(location_features.keys())[:5]\n",
    "    for i, loc in enumerate(sample_locations, 1):\n",
    "        n_samples = location_features[loc]['n_samples']\n",
    "        print(f\"   {i}. Location: {loc}\")\n",
    "        print(f\"      - Training samples: {n_samples}\")\n",
    "    \n",
    "    if len(location_features) > 5:\n",
    "        print(f\"   ... and {len(location_features) - 5} more locations\")\n",
    "else:\n",
    "    print(f\"\\n Location Data:\")\n",
    "    print(f\"    Single 'global' location created\")\n",
    "    print(f\"     All training data treated as one location\")\n",
    "    print(f\"     In production, all predictions will use same historical features\")\n",
    "\n",
    "print(f\"\\n Sample Time-Series Features:\")\n",
    "first_loc = list(location_features.keys())[0]\n",
    "sample_features = list(location_features[first_loc]['features'].items())[:10]\n",
    "\n",
    "for i, (feature, value) in enumerate(sample_features, 1):\n",
    "    feature_display = feature[:50] + '...' if len(feature) > 50 else feature\n",
    "    print(f\"   {i:2d}. {feature_display:<53} {value:>10.4f}\")\n",
    "\n",
    "if len(location_features[first_loc]['features']) > 10:\n",
    "    print(f\"   ... and {len(location_features[first_loc]['features']) - 10} more features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
