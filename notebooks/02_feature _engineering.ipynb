{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Air Quality PM2.5 Prediction - Feature Engineering\n",
    "\n",
    "**Author:** Chukwuahachie Sylvester \n",
    "**Date:** December 2, 2024  \n",
    "**Purpose:** Advanced feature engineering for PM2.5 prediction model\n",
    "\n",
    "## Objectives\n",
    "1. Handle missing data with domain knowledge\n",
    "2. Create domain-specific features (pollutant interactions, wind features)\n",
    "3. Engineer temporal features (seasonality, time patterns)\n",
    "4. Build aggregated features (rolling averages, city statistics)\n",
    "5. Apply dimensionality reduction (PCA on sensor angles)\n",
    "6. Create pollution clusters\n",
    "7. Prepare final dataset for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Statistical\n",
    "from scipy import stats\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = '../data/raw/air_quality.csv' \n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\"*60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INITIAL DATA PREPARATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: INITIAL DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Convert Date to datetime\n",
    "df_fe['Date'] = pd.to_datetime(df_fe['Date'])\n",
    "print(f\"\\n Date converted to datetime\")\n",
    "\n",
    "# Sort by Place and Date (important for time series features)\n",
    "df_fe = df_fe.sort_values(['Place_ID', 'Date']).reset_index(drop=True)\n",
    "print(f\" Data sorted by Place_ID and Date\")\n",
    "\n",
    "# Display info\n",
    "print(f\"\\n Dataset after preparation:\")\n",
    "print(f\"Shape: {df_fe.shape}\")\n",
    "print(f\"Date range: {df_fe['Date'].min()} to {df_fe['Date'].max()}\")\n",
    "print(f\"Number of unique places: {df_fe['Place_ID'].nunique()}\")\n",
    "\n",
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REMOVING DATA LEAKAGE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# These features are derived from target - they leak information!\n",
    "leakage_features = [\n",
    "    'target_min',\n",
    "    'target_max', \n",
    "    'target_variance',\n",
    "    'target_count'\n",
    "]\n",
    "\n",
    "# Check which exist\n",
    "existing_leakage = [col for col in leakage_features if col in df_fe.columns]\n",
    "\n",
    "if existing_leakage:\n",
    "    print(f\"\\n Removing {len(existing_leakage)} leakage features:\")\n",
    "    for feature in existing_leakage:\n",
    "        print(f\"   - {feature}\")\n",
    "    \n",
    "    df_fe = df_fe.drop(columns=existing_leakage)\n",
    "    print(f\"\\n Leakage features removed\")\n",
    "    print(f\"New shape: {df_fe.shape}\")\n",
    "else:\n",
    "    print(\"\\n No leakage features found\")\n",
    "\n",
    "# Also remove identifier columns (will add back later)\n",
    "id_cols = ['Place_ID X Date']\n",
    "if 'Place_ID X Date' in df_fe.columns:\n",
    "    df_fe = df_fe.drop(columns=id_cols)\n",
    "    print(f\" Removed identifier column\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: ANALYZE MISSING DATA PATTERNS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate missing percentages\n",
    "missing_pct = (df_fe.isnull().sum() / len(df_fe) * 100).sort_values(ascending=False)\n",
    "missing_pct = missing_pct[missing_pct > 0]\n",
    "\n",
    "print(f\"\\n Features with missing data: {len(missing_pct)}\")\n",
    "\n",
    "# Categorize by missing percentage\n",
    "high_missing = missing_pct[missing_pct > 50]\n",
    "medium_missing = missing_pct[(missing_pct > 10) & (missing_pct <= 50)]\n",
    "low_missing = missing_pct[missing_pct <= 10]\n",
    "\n",
    "print(f\"\\n HIGH missing (>50%): {len(high_missing)} features\")\n",
    "if len(high_missing) > 0:\n",
    "    print(high_missing.head(10))\n",
    "\n",
    "print(f\"\\n MEDIUM missing (10-50%): {len(medium_missing)} features\")\n",
    "if len(medium_missing) > 0:\n",
    "    print(medium_missing.head(10))\n",
    "\n",
    "print(f\"\\n LOW missing (<10%): {len(low_missing)} features\")\n",
    "if len(low_missing) > 0:\n",
    "    print(low_missing.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "missing_pct.head(30).plot(kind='barh', color='coral', edgecolor='black')\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.title('Top 30 Features with Missing Data', fontweight='bold', fontsize=14)\n",
    "plt.axvline(x=50, color='red', linestyle='--', linewidth=2, label='50% threshold')\n",
    "plt.axvline(x=10, color='orange', linestyle='--', linewidth=2, label='10% threshold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: DROP FEATURES WITH >80% MISSING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DROPPING HIGH-MISSING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Drop features with >80% missing (CH4 features)\n",
    "threshold = 80\n",
    "high_missing_cols = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "print(f\"\\n Dropping {len(high_missing_cols)} features with >{threshold}% missing:\")\n",
    "for col in high_missing_cols:\n",
    "    print(f\"   - {col}: {missing_pct[col]:.1f}% missing\")\n",
    "\n",
    "df_fe = df_fe.drop(columns=high_missing_cols)\n",
    "\n",
    "print(f\"\\n High-missing features dropped\")\n",
    "print(f\"New shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: CREATE TEMPORAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING TEMPORAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract temporal components\n",
    "df_fe['year'] = df_fe['Date'].dt.year\n",
    "df_fe['month'] = df_fe['Date'].dt.month\n",
    "df_fe['day'] = df_fe['Date'].dt.day\n",
    "df_fe['dayofweek'] = df_fe['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df_fe['dayofyear'] = df_fe['Date'].dt.dayofyear\n",
    "df_fe['week'] = df_fe['Date'].dt.isocalendar().week\n",
    "df_fe['quarter'] = df_fe['Date'].dt.quarter\n",
    "\n",
    "# Weekend indicator\n",
    "df_fe['is_weekend'] = (df_fe['dayofweek'] >= 5).astype(int)\n",
    "\n",
    "# Season (Northern Hemisphere)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_fe['season'] = df_fe['month'].apply(get_season)\n",
    "\n",
    "# Cyclical encoding for month (preserves cyclical nature)\n",
    "df_fe['month_sin'] = np.sin(2 * np.pi * df_fe['month'] / 12)\n",
    "df_fe['month_cos'] = np.cos(2 * np.pi * df_fe['month'] / 12)\n",
    "\n",
    "# Cyclical encoding for day of week\n",
    "df_fe['dayofweek_sin'] = np.sin(2 * np.pi * df_fe['dayofweek'] / 7)\n",
    "df_fe['dayofweek_cos'] = np.cos(2 * np.pi * df_fe['dayofweek'] / 7)\n",
    "\n",
    "print(f\"\\n Created {8 + 4} temporal features:\")\n",
    "print(f\"   - Basic: year, month, day, dayofweek, dayofyear, week, quarter, season\")\n",
    "print(f\"   - Binary: is_weekend\")\n",
    "print(f\"   - Cyclical: month_sin/cos, dayofweek_sin/cos\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n Sample temporal features:\")\n",
    "df_fe[['Date', 'month', 'season', 'is_weekend', 'month_sin', 'month_cos']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: CREATE WIND FEATURES (DOMAIN KNOWLEDGE)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING WIND FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Wind speed from u and v components\n",
    "# Formula: wind_speed = sqrt(u² + v²)\n",
    "if 'u_component_of_wind_10m_above_ground' in df_fe.columns and 'v_component_of_wind_10m_above_ground' in df_fe.columns:\n",
    "    \n",
    "    df_fe['wind_speed'] = np.sqrt(\n",
    "        df_fe['u_component_of_wind_10m_above_ground']**2 + \n",
    "        df_fe['v_component_of_wind_10m_above_ground']**2\n",
    "    )\n",
    "    \n",
    "    # Wind direction (in radians, then convert to degrees)\n",
    "    df_fe['wind_direction'] = np.arctan2(\n",
    "        df_fe['v_component_of_wind_10m_above_ground'],\n",
    "        df_fe['u_component_of_wind_10m_above_ground']\n",
    "    )\n",
    "    \n",
    "    # Convert to degrees (0-360)\n",
    "    df_fe['wind_direction_degrees'] = (df_fe['wind_direction'] * 180 / np.pi) % 360\n",
    "    \n",
    "    # Wind direction categories (N, NE, E, SE, S, SW, W, NW)\n",
    "    def categorize_wind_direction(degrees):\n",
    "        if pd.isna(degrees):\n",
    "            return np.nan\n",
    "        elif degrees < 22.5 or degrees >= 337.5:\n",
    "            return 'N'\n",
    "        elif degrees < 67.5:\n",
    "            return 'NE'\n",
    "        elif degrees < 112.5:\n",
    "            return 'E'\n",
    "        elif degrees < 157.5:\n",
    "            return 'SE'\n",
    "        elif degrees < 202.5:\n",
    "            return 'S'\n",
    "        elif degrees < 247.5:\n",
    "            return 'SW'\n",
    "        elif degrees < 292.5:\n",
    "            return 'W'\n",
    "        else:\n",
    "            return 'NW'\n",
    "    \n",
    "    df_fe['wind_direction_category'] = df_fe['wind_direction_degrees'].apply(categorize_wind_direction)\n",
    "    \n",
    "    print(f\"\\n Created wind features:\")\n",
    "    print(f\"   - wind_speed (magnitude)\")\n",
    "    print(f\"   - wind_direction (radians)\")\n",
    "    print(f\"   - wind_direction_degrees (0-360°)\")\n",
    "    print(f\"   - wind_direction_category (N, NE, E, etc.)\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Wind speed distribution\n",
    "    axes[0].hist(df_fe['wind_speed'].dropna(), bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0].set_xlabel('Wind Speed (m/s)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Wind Speed Distribution', fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Wind direction\n",
    "    wind_dir_counts = df_fe['wind_direction_category'].value_counts()\n",
    "    axes[1].bar(wind_dir_counts.index, wind_dir_counts.values, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].set_xlabel('Wind Direction')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Wind Direction Distribution', fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Wind component features not found\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 7: CREATE POLLUTANT INTERACTION FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING POLLUTANT INTERACTION FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Key pollutants\n",
    "pollutants = {\n",
    "    'CO': 'L3_CO_CO_column_number_density',\n",
    "    'NO2': 'L3_NO2_NO2_column_number_density',\n",
    "    'SO2': 'L3_SO2_SO2_column_number_density',\n",
    "    'HCHO': 'L3_HCHO_tropospheric_HCHO_column_number_density'\n",
    "}\n",
    "\n",
    "# Check which exist\n",
    "existing_pollutants = {k: v for k, v in pollutants.items() if v in df_fe.columns}\n",
    "\n",
    "print(f\"\\n Found {len(existing_pollutants)} pollutant features:\")\n",
    "for name, col in existing_pollutants.items():\n",
    "    print(f\"   - {name}: {col}\")\n",
    "\n",
    "if len(existing_pollutants) >= 2:\n",
    "    \n",
    "    pollutant_cols = list(existing_pollutants.values())\n",
    "    \n",
    "    # 1. Total pollutant load (sum of all pollutants)\n",
    "    df_fe['total_pollutant_load'] = df_fe[pollutant_cols].sum(axis=1)\n",
    "    \n",
    "    # 2. Average pollutant concentration\n",
    "    df_fe['avg_pollutant_concentration'] = df_fe[pollutant_cols].mean(axis=1)\n",
    "    \n",
    "    # 3. Pollutant interaction: CO × NO2 (vehicle emissions proxy)\n",
    "    if 'L3_CO_CO_column_number_density' in df_fe.columns and 'L3_NO2_NO2_column_number_density' in df_fe.columns:\n",
    "        df_fe['CO_NO2_interaction'] = df_fe['L3_CO_CO_column_number_density'] * df_fe['L3_NO2_NO2_column_number_density']\n",
    "    \n",
    "    # 4. Air Quality Index proxy (weighted sum)\n",
    "    # Weights based on health impact\n",
    "    if all(col in df_fe.columns for col in pollutant_cols):\n",
    "        df_fe['AQI_proxy'] = (\n",
    "            0.4 * df_fe.get('L3_CO_CO_column_number_density', 0) +\n",
    "            0.3 * df_fe.get('L3_NO2_NO2_column_number_density', 0) +\n",
    "            0.2 * df_fe.get('L3_SO2_SO2_column_number_density', 0) +\n",
    "            0.1 * df_fe.get('L3_HCHO_tropospheric_HCHO_column_number_density', 0)\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nCreated {4} pollutant interaction features:\")\n",
    "    print(f\"   - total_pollutant_load\")\n",
    "    print(f\"   - avg_pollutant_concentration\")\n",
    "    print(f\"   - CO_NO2_interaction\")\n",
    "    print(f\"   - AQI_proxy\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Not enough pollutant features for interactions\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: CREATE ATMOSPHERIC STABILITY FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING ATMOSPHERIC STABILITY FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temperature-Humidity Index (measures atmospheric comfort/stability)\n",
    "if 'temperature_2m_above_ground' in df_fe.columns and 'relative_humidity_2m_above_ground' in df_fe.columns:\n",
    "    \n",
    "    # Heat Index (how hot it feels)\n",
    "    df_fe['heat_index'] = df_fe['temperature_2m_above_ground'] + (\n",
    "        0.5 * df_fe['relative_humidity_2m_above_ground'] / 100\n",
    "    )\n",
    "    \n",
    "    # Temperature-Pressure interaction (atmospheric stability)\n",
    "    if 'L3_NO2_tropopause_pressure' in df_fe.columns:\n",
    "        df_fe['temp_pressure_ratio'] = (\n",
    "            df_fe['temperature_2m_above_ground'] / \n",
    "            (df_fe['L3_NO2_tropopause_pressure'] + 1)  # +1 to avoid division by zero\n",
    "        )\n",
    "    \n",
    "    # Humidity categories\n",
    "    def categorize_humidity(humidity):\n",
    "        if pd.isna(humidity):\n",
    "            return np.nan\n",
    "        elif humidity < 30:\n",
    "            return 'Dry'\n",
    "        elif humidity < 60:\n",
    "            return 'Moderate'\n",
    "        else:\n",
    "            return 'Humid'\n",
    "    \n",
    "    df_fe['humidity_category'] = df_fe['relative_humidity_2m_above_ground'].apply(categorize_humidity)\n",
    "    \n",
    "    # Temperature-Humidity interaction\n",
    "    df_fe['temp_humidity_interaction'] = (\n",
    "        df_fe['temperature_2m_above_ground'] * \n",
    "        df_fe['relative_humidity_2m_above_ground']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Created atmospheric stability features:\")\n",
    "    print(f\"   - heat_index\")\n",
    "    print(f\"   - temp_pressure_ratio\")\n",
    "    print(f\"   - humidity_category\")\n",
    "    print(f\"   - temp_humidity_interaction\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Temperature/Humidity features not found\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 9: CREATE LOCATION-BASED AGGREGATED FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING LOCATION-BASED AGGREGATED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate statistics by Place_ID\n",
    "if 'Place_ID' in df_fe.columns:\n",
    "    \n",
    "    # Mean PM2.5 per location (pollution history)\n",
    "    place_avg_pm25 = df_fe.groupby('Place_ID')['target'].transform('mean')\n",
    "    df_fe['place_avg_pm25'] = place_avg_pm25\n",
    "    \n",
    "    # Standard deviation per location (pollution variability)\n",
    "    place_std_pm25 = df_fe.groupby('Place_ID')['target'].transform('std')\n",
    "    df_fe['place_std_pm25'] = place_std_pm25\n",
    "    \n",
    "    # Median per location\n",
    "    place_median_pm25 = df_fe.groupby('Place_ID')['target'].transform('median')\n",
    "    df_fe['place_median_pm25'] = place_median_pm25\n",
    "    \n",
    "    # Min and Max per location\n",
    "    place_min_pm25 = df_fe.groupby('Place_ID')['target'].transform('min')\n",
    "    df_fe['place_min_pm25'] = place_min_pm25\n",
    "    \n",
    "    place_max_pm25 = df_fe.groupby('Place_ID')['target'].transform('max')\n",
    "    df_fe['place_max_pm25'] = place_max_pm25\n",
    "    \n",
    "    # Number of observations per location\n",
    "    place_count = df_fe.groupby('Place_ID')['target'].transform('count')\n",
    "    df_fe['place_observation_count'] = place_count\n",
    "    \n",
    "    print(f\"\\n Created {6} location-based features:\")\n",
    "    print(f\"   - place_avg_pm25 (historical average)\")\n",
    "    print(f\"   - place_std_pm25 (variability)\")\n",
    "    print(f\"   - place_median_pm25\")\n",
    "    print(f\"   - place_min_pm25\")\n",
    "    print(f\"   - place_max_pm25\")\n",
    "    print(f\"   - place_observation_count\")\n",
    "    \n",
    "    # Visualize top polluted locations\n",
    "    place_pollution = df_fe.groupby('Place_ID')['target'].mean().sort_values(ascending=False).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    place_pollution.plot(kind='barh', color='crimson', edgecolor='black')\n",
    "    plt.xlabel('Average PM2.5 (μg/m³)')\n",
    "    plt.ylabel('Place ID')\n",
    "    plt.title('Top 15 Most Polluted Locations', fontweight='bold', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Place_ID column not found\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 10: CREATE ROLLING WINDOW FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING ROLLING WINDOW FEATURES (TIME SERIES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by Place_ID and Date (critical for rolling windows)\n",
    "df_fe = df_fe.sort_values(['Place_ID', 'Date']).reset_index(drop=True)\n",
    "\n",
    "if 'Place_ID' in df_fe.columns and 'Date' in df_fe.columns:\n",
    "    \n",
    "    print(\"\\n Creating rolling averages per location...\")\n",
    "    \n",
    "    # 3-day rolling average\n",
    "    df_fe['pm25_rolling_3day'] = df_fe.groupby('Place_ID')['target'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # 7-day rolling average\n",
    "    df_fe['pm25_rolling_7day'] = df_fe.groupby('Place_ID')['target'].transform(\n",
    "        lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # 14-day rolling average\n",
    "    df_fe['pm25_rolling_14day'] = df_fe.groupby('Place_ID')['target'].transform(\n",
    "        lambda x: x.rolling(window=14, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # 30-day rolling average\n",
    "    df_fe['pm25_rolling_30day'] = df_fe.groupby('Place_ID')['target'].transform(\n",
    "        lambda x: x.rolling(window=30, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Rolling standard deviation (volatility)\n",
    "    df_fe['pm25_rolling_std_7day'] = df_fe.groupby('Place_ID')['target'].transform(\n",
    "        lambda x: x.rolling(window=7, min_periods=1).std()\n",
    "    )\n",
    "    \n",
    "    # Lag features (yesterday's pollution)\n",
    "    df_fe['pm25_lag_1day'] = df_fe.groupby('Place_ID')['target'].shift(1)\n",
    "    df_fe['pm25_lag_7day'] = df_fe.groupby('Place_ID')['target'].shift(7)\n",
    "    \n",
    "    # Difference from yesterday (trend)\n",
    "    df_fe['pm25_diff_1day'] = df_fe['target'] - df_fe['pm25_lag_1day']\n",
    "    \n",
    "    # Difference from 7 days ago (weekly trend)\n",
    "    df_fe['pm25_diff_7day'] = df_fe['target'] - df_fe['pm25_lag_7day']\n",
    "    \n",
    "    print(f\"\\n Created {9} rolling window features:\")\n",
    "    print(f\"   - pm25_rolling_3day, 7day, 14day, 30day\")\n",
    "    print(f\"   - pm25_rolling_std_7day\")\n",
    "    print(f\"   - pm25_lag_1day, pm25_lag_7day\")\n",
    "    print(f\"   - pm25_diff_1day, pm25_diff_7day\")\n",
    "    \n",
    "    # Visualize for one location\n",
    "    sample_place = df_fe['Place_ID'].iloc[0]\n",
    "    sample_data = df_fe[df_fe['Place_ID'] == sample_place].head(60)\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(sample_data['Date'], sample_data['target'], 'o-', alpha=0.5, label='Actual PM2.5')\n",
    "    plt.plot(sample_data['Date'], sample_data['pm25_rolling_7day'], linewidth=2, label='7-day Rolling Avg')\n",
    "    plt.plot(sample_data['Date'], sample_data['pm25_rolling_30day'], linewidth=2, label='30-day Rolling Avg')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('PM2.5 (μg/m³)')\n",
    "    plt.title(f'Rolling Averages - {sample_place}', fontweight='bold', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Cannot create rolling features without Place_ID and Date\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 11: CREATE POLYNOMIAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING POLYNOMIAL FEATURES FOR TOP PREDICTORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Top correlated features from EDA\n",
    "top_features = [\n",
    "    'L3_CO_CO_column_number_density',\n",
    "    'L3_HCHO_tropospheric_HCHO_column_number_density',\n",
    "    'L3_NO2_NO2_column_number_density',\n",
    "    'temperature_2m_above_ground',\n",
    "    'relative_humidity_2m_above_ground'\n",
    "]\n",
    "\n",
    "# Filter to existing features\n",
    "top_features = [f for f in top_features if f in df_fe.columns]\n",
    "\n",
    "print(f\"\\n Creating polynomial features for {len(top_features)} top predictors:\")\n",
    "for feature in top_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "for feature in top_features:\n",
    "    # Square\n",
    "    df_fe[f'{feature}_squared'] = df_fe[feature] ** 2\n",
    "    \n",
    "    # Square root (for positive values)\n",
    "    if df_fe[feature].min() >= 0:\n",
    "        df_fe[f'{feature}_sqrt'] = np.sqrt(df_fe[feature])\n",
    "    \n",
    "    # Log transform (for positive values)\n",
    "    if df_fe[feature].min() > 0:\n",
    "        df_fe[f'{feature}_log'] = np.log1p(df_fe[feature])\n",
    "\n",
    "print(f\"\\n Created polynomial features (squared, sqrt, log)\")\n",
    "print(f\"New shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 12: CREATE RATIO FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING RATIO FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temperature to humidity ratio (dryness index)\n",
    "if 'temperature_2m_above_ground' in df_fe.columns and 'relative_humidity_2m_above_ground' in df_fe.columns:\n",
    "    df_fe['temp_humidity_ratio'] = (\n",
    "        df_fe['temperature_2m_above_ground'] / \n",
    "        (df_fe['relative_humidity_2m_above_ground'] + 1)\n",
    "    )\n",
    "\n",
    "# NO2 to CO ratio (traffic vs industrial pollution)\n",
    "if 'L3_NO2_NO2_column_number_density' in df_fe.columns and 'L3_CO_CO_column_number_density' in df_fe.columns:\n",
    "    df_fe['NO2_CO_ratio'] = (\n",
    "        df_fe['L3_NO2_NO2_column_number_density'] / \n",
    "        (df_fe['L3_CO_CO_column_number_density'] + 1e-6)\n",
    "    )\n",
    "\n",
    "# Pollutant concentration per wind speed (dispersion efficiency)\n",
    "if 'total_pollutant_load' in df_fe.columns and 'wind_speed' in df_fe.columns:\n",
    "    df_fe['pollutant_per_windspeed'] = (\n",
    "        df_fe['total_pollutant_load'] / \n",
    "        (df_fe['wind_speed'] + 0.1)  # Avoid division by zero\n",
    "    )\n",
    "\n",
    "print(f\"\\n Created ratio features:\")\n",
    "print(f\"   - temp_humidity_ratio\")\n",
    "print(f\"   - NO2_CO_ratio\")\n",
    "print(f\"   - pollutant_per_windspeed\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 13: APPLY PCA FOR DIMENSIONALITY REDUCTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"APPLYING PCA FOR DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all sensor angle features (highly correlated, low predictive power)\n",
    "angle_features = [col for col in df_fe.columns if 'angle' in col.lower() and col in df_fe.select_dtypes(include=[np.number]).columns]\n",
    "\n",
    "print(f\"\\n Found {len(angle_features)} sensor angle features:\")\n",
    "for feature in angle_features[:10]:\n",
    "    print(f\"   - {feature}\")\n",
    "if len(angle_features) > 10:\n",
    "    print(f\"   ... and {len(angle_features) - 10} more\")\n",
    "\n",
    "if len(angle_features) >= 5:\n",
    "    \n",
    "    print(f\"\\n Reducing {len(angle_features)} angle features to 5 principal components...\")\n",
    "    \n",
    "    # Prepare data for PCA (handle missing values)\n",
    "    angle_data = df_fe[angle_features].copy()\n",
    "    \n",
    "    # Impute missing values with median\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    angle_data_imputed = imputer.fit_transform(angle_data)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    angle_data_scaled = scaler.fit_transform(angle_data_imputed)\n",
    "    \n",
    "    # Apply PCA\n",
    "    n_components = min(5, len(angle_features))\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    angle_pca = pca.fit_transform(angle_data_scaled)\n",
    "    \n",
    "    # Add PCA components to dataframe\n",
    "    for i in range(n_components):\n",
    "        df_fe[f'angle_pca_{i+1}'] = angle_pca[:, i]\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumulative_var = np.cumsum(explained_var)\n",
    "    \n",
    "    print(f\"\\n PCA completed!\")\n",
    "    print(f\"   - Created {n_components} PCA components\")\n",
    "    print(f\"   - Explained variance: {explained_var}\")\n",
    "    print(f\"   - Cumulative variance: {cumulative_var}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(1, n_components+1), explained_var, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA - Explained Variance per Component', fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, n_components+1), cumulative_var, 'o-', linewidth=2, markersize=8, color='coral')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA - Cumulative Explained Variance', fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Drop original angle features (optional - keep for now)\n",
    "    # df_fe = df_fe.drop(columns=angle_features)\n",
    "    # print(f\"\\n Dropped {len(angle_features)} original angle features\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n Not enough angle features for PCA (found {len(angle_features)}, need at least 5)\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 14: CREATE POLLUTION LEVEL CLUSTERS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING POLLUTION LEVEL CLUSTERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Features for clustering\n",
    "cluster_features = [\n",
    "    'target',\n",
    "    'L3_CO_CO_column_number_density',\n",
    "    'L3_NO2_NO2_column_number_density',\n",
    "    'temperature_2m_above_ground',\n",
    "    'relative_humidity_2m_above_ground',\n",
    "    'wind_speed'\n",
    "]\n",
    "\n",
    "# Filter to existing features\n",
    "cluster_features = [f for f in cluster_features if f in df_fe.columns]\n",
    "\n",
    "print(f\"\\n Using {len(cluster_features)} features for clustering:\")\n",
    "for feature in cluster_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "if len(cluster_features) >= 3:\n",
    "    \n",
    "    # Prepare data\n",
    "    cluster_data = df_fe[cluster_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    cluster_data_imputed = imputer.fit_transform(cluster_data)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data_scaled = scaler.fit_transform(cluster_data_imputed)\n",
    "    \n",
    "    # Apply K-Means clustering\n",
    "    n_clusters = 4  # Low, Medium, High, Very High pollution zones\n",
    "    \n",
    "    print(f\"\\n Applying K-Means clustering with {n_clusters} clusters...\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df_fe['pollution_cluster'] = kmeans.fit_predict(cluster_data_scaled)\n",
    "    \n",
    "    print(f\"\\n Clustering completed!\")\n",
    "    \n",
    "    # Analyze clusters\n",
    "    print(f\"\\n Cluster Analysis:\")\n",
    "    cluster_stats = df_fe.groupby('pollution_cluster')['target'].agg(['mean', 'median', 'std', 'count'])\n",
    "    cluster_stats = cluster_stats.sort_values('mean')\n",
    "    cluster_stats.index = [f'Cluster {i}' for i in cluster_stats.index]\n",
    "    print(cluster_stats)\n",
    "    \n",
    "    # Rename clusters based on pollution level\n",
    "    cluster_mapping = dict(zip(\n",
    "        cluster_stats.sort_values('mean').index,\n",
    "        ['Low', 'Medium', 'High', 'Very High']\n",
    "    ))\n",
    "    \n",
    "    # Visualize clusters\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Cluster distribution\n",
    "    cluster_counts = df_fe['pollution_cluster'].value_counts().sort_index()\n",
    "    axes[0].bar(cluster_counts.index, cluster_counts.values, edgecolor='black', alpha=0.7, color='teal')\n",
    "    axes[0].set_xlabel('Cluster')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Pollution Cluster Distribution', fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot of PM2.5 by cluster\n",
    "    df_fe.boxplot(column='target', by='pollution_cluster', ax=axes[1], patch_artist=True)\n",
    "    axes[1].set_xlabel('Pollution Cluster')\n",
    "    axes[1].set_ylabel('PM2.5 (μg/m³)')\n",
    "    axes[1].set_title('PM2.5 Distribution by Cluster', fontweight='bold')\n",
    "    axes[1].get_figure().suptitle('')  # Remove auto title\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n Not enough features for clustering\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 16: TRANSFORM TARGET VARIABLE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMING TARGET VARIABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze target distribution before transformation\n",
    "print(\"\\n Target Variable (PM2.5) - Before Transformation:\")\n",
    "print(f\"Mean: {df_fe['target'].mean():.2f}\")\n",
    "print(f\"Median: {df_fe['target'].median():.2f}\")\n",
    "print(f\"Std: {df_fe['target'].std():.2f}\")\n",
    "print(f\"Skewness: {df_fe['target'].skew():.3f}\")\n",
    "print(f\"Kurtosis: {df_fe['target'].kurtosis():.3f}\")\n",
    "print(f\"Min: {df_fe['target'].min():.2f}\")\n",
    "print(f\"Max: {df_fe['target'].max():.2f}\")\n",
    "\n",
    "# Visualize before transformation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(df_fe['target'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 0].set_xlabel('PM2.5 (μg/m³)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Original Target Distribution', fontweight='bold')\n",
    "axes[0, 0].axvline(df_fe['target'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0, 0].axvline(df_fe['target'].median(), color='blue', linestyle='--', linewidth=2, label='Median')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Original Q-Q plot\n",
    "stats.probplot(df_fe['target'], dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot - Original', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Apply log transformation\n",
    "# Using log1p to handle zeros: log1p(x) = log(1 + x)\n",
    "df_fe['target_log'] = np.log1p(df_fe['target'])\n",
    "\n",
    "print(f\"\\n Log transformation applied: target_log = log(1 + target)\")\n",
    "\n",
    "# Analyze after transformation\n",
    "print(\"\\n Target Variable - After Log Transformation:\")\n",
    "print(f\"Mean: {df_fe['target_log'].mean():.2f}\")\n",
    "print(f\"Median: {df_fe['target_log'].median():.2f}\")\n",
    "print(f\"Std: {df_fe['target_log'].std():.2f}\")\n",
    "print(f\"Skewness: {df_fe['target_log'].skew():.3f}\")\n",
    "print(f\"Kurtosis: {df_fe['target_log'].kurtosis():.3f}\")\n",
    "\n",
    "# Log-transformed distribution\n",
    "axes[1, 0].hist(df_fe['target_log'], bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_xlabel('log(PM2.5 + 1)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Log-Transformed Target Distribution', fontweight='bold')\n",
    "axes[1, 0].axvline(df_fe['target_log'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[1, 0].axvline(df_fe['target_log'].median(), color='blue', linestyle='--', linewidth=2, label='Median')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Log-transformed Q-Q plot\n",
    "stats.probplot(df_fe['target_log'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot - Log-Transformed', fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare skewness reduction\n",
    "skew_reduction = ((df_fe['target'].skew() - df_fe['target_log'].skew()) / df_fe['target'].skew()) * 100\n",
    "print(f\"\\n Skewness reduced by {skew_reduction:.1f}%\")\n",
    "print(f\"   Original skewness: {df_fe['target'].skew():.3f}\")\n",
    "print(f\"   Log skewness: {df_fe['target_log'].skew():.3f}\")\n",
    "\n",
    "if abs(df_fe['target_log'].skew()) < 0.5:\n",
    "    print(f\"\\n Distribution is now approximately normal!\")\n",
    "else:\n",
    "    print(f\"\\n Distribution still skewed but improved\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 17: HANDLE OUTLIERS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HANDLING OUTLIERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Function to cap outliers\n",
    "def cap_outliers(series, lower_bound, upper_bound):\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Get numerical features (excluding target)\n",
    "numerical_features = df_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "features_to_exclude = ['target', 'target_log', 'Date', 'year', 'month', 'day', 'dayofweek']\n",
    "numerical_features = [f for f in numerical_features if f not in features_to_exclude]\n",
    "\n",
    "print(f\"\\n Analyzing outliers in {len(numerical_features)} numerical features...\")\n",
    "\n",
    "# Track outlier statistics\n",
    "outlier_stats = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    # Count outliers before\n",
    "    lower, upper = detect_outliers_iqr(df_fe[feature])\n",
    "    outliers_before = ((df_fe[feature] < lower) | (df_fe[feature] > upper)).sum()\n",
    "    outliers_pct_before = (outliers_before / len(df_fe)) * 100\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        outlier_stats.append({\n",
    "            'Feature': feature,\n",
    "            'Outliers_Before': outliers_before,\n",
    "            'Outliers_Pct': outliers_pct_before,\n",
    "            'Lower_Bound': lower,\n",
    "            'Upper_Bound': upper\n",
    "        })\n",
    "\n",
    "# Sort by percentage\n",
    "outlier_df = pd.DataFrame(outlier_stats).sort_values('Outliers_Pct', ascending=False)\n",
    "\n",
    "print(f\"\\n Top 15 Features with Most Outliers:\")\n",
    "print(outlier_df.head(15)[['Feature', 'Outliers_Before', 'Outliers_Pct']])\n",
    "\n",
    "# Visualize top outlier features\n",
    "top_outlier_features = outlier_df.head(6)['Feature'].tolist()\n",
    "\n",
    "if len(top_outlier_features) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_outlier_features):\n",
    "        # Before capping\n",
    "        axes[idx].boxplot(df_fe[feature].dropna(), vert=True, patch_artist=True)\n",
    "        axes[idx].set_ylabel(feature[:30] + '...' if len(feature) > 30 else feature)\n",
    "        axes[idx].set_title(f'Outliers: {feature[:25]}...', fontweight='bold', fontsize=10)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply outlier capping strategy\n",
    "print(f\"\\n Capping outliers using IQR method (1.5 * IQR)...\")\n",
    "\n",
    "outliers_capped_count = 0\n",
    "\n",
    "for feature in numerical_features:\n",
    "    lower, upper = detect_outliers_iqr(df_fe[feature])\n",
    "    outliers_before = ((df_fe[feature] < lower) | (df_fe[feature] > upper)).sum()\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        # Cap outliers\n",
    "        df_fe[feature] = cap_outliers(df_fe[feature], lower, upper)\n",
    "        \n",
    "        # Count remaining outliers\n",
    "        outliers_after = ((df_fe[feature] < lower) | (df_fe[feature] > upper)).sum()\n",
    "        outliers_capped_count += (outliers_before - outliers_after)\n",
    "\n",
    "print(f\"\\n Outlier handling completed!\")\n",
    "print(f\"   Total outliers capped: {outliers_capped_count:,}\")\n",
    "\n",
    "# Visualize after capping\n",
    "if len(top_outlier_features) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_outlier_features):\n",
    "        # After capping\n",
    "        axes[idx].boxplot(df_fe[feature].dropna(), vert=True, patch_artist=True, \n",
    "                         boxprops=dict(facecolor='lightgreen'))\n",
    "        axes[idx].set_ylabel(feature[:30] + '...' if len(feature) > 30 else feature)\n",
    "        axes[idx].set_title(f'After Capping: {feature[:25]}...', fontweight='bold', fontsize=10)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 18: ENCODE CATEGORICAL FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = df_fe.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\n Found {len(categorical_features)} categorical features:\")\n",
    "for feature in categorical_features:\n",
    "    unique_count = df_fe[feature].nunique()\n",
    "    print(f\"   - {feature}: {unique_count} unique values\")\n",
    "\n",
    "# Encode categorical features\n",
    "if len(categorical_features) > 0:\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        unique_count = df_fe[feature].nunique()\n",
    "        \n",
    "        # Strategy 1: Label Encoding for ordinal or low cardinality\n",
    "        if unique_count <= 10:\n",
    "            print(f\"\\n Label Encoding: {feature} ({unique_count} categories)\")\n",
    "            le = LabelEncoder()\n",
    "            df_fe[f'{feature}_encoded'] = le.fit_transform(df_fe[feature].astype(str))\n",
    "            \n",
    "            # Show mapping\n",
    "            mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            print(f\"   Mapping: {mapping}\")\n",
    "        \n",
    "        # Strategy 2: One-Hot Encoding for nominal with moderate cardinality\n",
    "        elif 10 < unique_count <= 50:\n",
    "            print(f\"\\n One-Hot Encoding: {feature} ({unique_count} categories)\")\n",
    "            # Get top 10 most frequent categories\n",
    "            top_categories = df_fe[feature].value_counts().head(10).index.tolist()\n",
    "            \n",
    "            for category in top_categories:\n",
    "                df_fe[f'{feature}_{category}'] = (df_fe[feature] == category).astype(int)\n",
    "            \n",
    "            print(f\"   Created {len(top_categories)} binary features\")\n",
    "        \n",
    "        # Strategy 3: Target encoding for high cardinality\n",
    "        else:\n",
    "            print(f\"\\n Target Encoding: {feature} ({unique_count} categories - high cardinality)\")\n",
    "            # Mean target value per category\n",
    "            target_means = df_fe.groupby(feature)['target'].mean()\n",
    "            df_fe[f'{feature}_target_encoded'] = df_fe[feature].map(target_means)\n",
    "            \n",
    "            print(f\"   Mapped to target mean values\")\n",
    "    \n",
    "    print(f\"\\n Categorical encoding completed!\")\n",
    "    \n",
    "    # Optional: Drop original categorical columns\n",
    "    # df_fe = df_fe.drop(columns=categorical_features)\n",
    "    # print(f\"   Dropped {len(categorical_features)} original categorical features\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n No categorical features to encode\")\n",
    "\n",
    "print(f\"\\nNew shape: {df_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 19: COMPREHENSIVE FEATURE SELECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data for feature selection\n",
    "# Get all numerical features (exclude IDs, dates, and original target)\n",
    "features_to_exclude = ['target', 'Date', 'Place_ID', 'Place_ID X Date']\n",
    "all_features = [col for col in df_fe.columns if col not in features_to_exclude]\n",
    "all_features = [col for col in all_features if df_fe[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"\\n Starting with {len(all_features)} potential features\")\n",
    "\n",
    "# Use log-transformed target for modeling\n",
    "X = df_fe[all_features].drop('target_log',axis = 1).copy()\n",
    "y = df_fe['target_log'].copy()\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"\\n Feature matrix:\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Features: {len(all_features)}\")\n",
    "print(f\"   Samples: {len(X)}\")\n",
    "\n",
    "# Split data for feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split:\")\n",
    "print(f\"   Training: {X_train.shape}\")\n",
    "print(f\"   Testing: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 1: VARIANCE THRESHOLD\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 1: VARIANCE THRESHOLD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low variance features (almost constant)\n",
    "variance_threshold = 0.01  # Features with variance < 0.01 are removed\n",
    "\n",
    "print(f\"\\n Removing features with variance < {variance_threshold}...\")\n",
    "\n",
    "# Standardize first (variance threshold works better on scaled data)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply variance threshold\n",
    "var_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_var = var_selector.fit_transform(X_train_scaled)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_var = X_train.columns[var_selector.get_support()].tolist()\n",
    "\n",
    "print(f\"\\n Variance Threshold completed!\")\n",
    "print(f\"   Features before: {X_train.shape[1]}\")\n",
    "print(f\"   Features after: {len(selected_features_var)}\")\n",
    "print(f\"   Features removed: {X_train.shape[1] - len(selected_features_var)}\")\n",
    "\n",
    "# Show removed features\n",
    "removed_features_var = [f for f in X_train.columns if f not in selected_features_var]\n",
    "if len(removed_features_var) > 0:\n",
    "    print(f\"\\n Removed low-variance features ({len(removed_features_var)}):\")\n",
    "    for feature in removed_features_var[:10]:\n",
    "        print(f\"   - {feature}\")\n",
    "    if len(removed_features_var) > 10:\n",
    "        print(f\"   ... and {len(removed_features_var) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 2: CORRELATION FILTER\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 2: CORRELATION FILTER (Remove Multicollinearity)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Work with variance-filtered features\n",
    "X_train_corr = X_train[selected_features_var].copy()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = X_train_corr.corr().abs()\n",
    "\n",
    "print(f\"\\n Finding highly correlated feature pairs...\")\n",
    "\n",
    "# Find pairs with correlation > 0.95\n",
    "high_corr_threshold = 0.95\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation > threshold\n",
    "to_drop_corr = [column for column in upper_triangle.columns \n",
    "                if any(upper_triangle[column] > high_corr_threshold)]\n",
    "\n",
    "print(f\"\\n Correlation filter completed!\")\n",
    "print(f\"   Features before: {len(selected_features_var)}\")\n",
    "print(f\"   Highly correlated features found: {len(to_drop_corr)}\")\n",
    "\n",
    "if len(to_drop_corr) > 0:\n",
    "    print(f\"\\n🗑️ Removing highly correlated features:\")\n",
    "    for feature in to_drop_corr[:15]:\n",
    "        print(f\"   - {feature}\")\n",
    "    if len(to_drop_corr) > 15:\n",
    "        print(f\"   ... and {len(to_drop_corr) - 15} more\")\n",
    "\n",
    "# Remove correlated features\n",
    "selected_features_corr = [f for f in selected_features_var if f not in to_drop_corr]\n",
    "print(f\"   Features after: {len(selected_features_corr)}\")\n",
    "\n",
    "# Visualize correlation matrix (top 30 features)\n",
    "top_features_viz = selected_features_corr[:30]\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(\n",
    "    X_train[top_features_viz].corr(),\n",
    "    annot=False,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title('Feature Correlation Matrix (Top 30 Features)', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 3: SELECTKBEST (F-STATISTIC & MUTUAL INFORMATION)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 3: SELECTKBEST - STATISTICAL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "# Work with correlation-filtered features\n",
    "X_train_kbest = X_train[selected_features_corr].copy()\n",
    "X_test_kbest = X_test[selected_features_corr].copy()\n",
    "\n",
    "# Method 3A: F-statistic (ANOVA)\n",
    "print(f\"\\n Method 3A: F-statistic (ANOVA F-test)...\")\n",
    "\n",
    "k_features = min(100, len(selected_features_corr))  # Select top 100 or all if less\n",
    "\n",
    "selector_f = SelectKBest(score_func=f_regression, k=k_features)\n",
    "selector_f.fit(X_train_kbest, y_train)\n",
    "\n",
    "# Get scores\n",
    "f_scores = pd.DataFrame({\n",
    "    'Feature': X_train_kbest.columns,\n",
    "    'F_Score': selector_f.scores_\n",
    "}).sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(f\"\\n F-statistic scoring completed!\")\n",
    "print(f\"\\n Top 15 Features by F-Score:\")\n",
    "print(f_scores.head(15))\n",
    "\n",
    "# Method 3B: Mutual Information\n",
    "print(f\"\\n Method 3B: Mutual Information...\")\n",
    "\n",
    "selector_mi = SelectKBest(score_func=mutual_info_regression, k=k_features)\n",
    "selector_mi.fit(X_train_kbest, y_train)\n",
    "\n",
    "# Get scores\n",
    "mi_scores = pd.DataFrame({\n",
    "    'Feature': X_train_kbest.columns,\n",
    "    'MI_Score': selector_mi.scores_\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(f\"\\n Mutual Information scoring completed!\")\n",
    "print(f\"\\n Top 15 Features by MI Score:\")\n",
    "print(mi_scores.head(15))\n",
    "\n",
    "# Combine scores (normalized)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_scores = MinMaxScaler()\n",
    "\n",
    "f_scores_norm = scaler_scores.fit_transform(f_scores[['F_Score']])\n",
    "mi_scores_norm = scaler_scores.fit_transform(mi_scores[['MI_Score']])\n",
    "\n",
    "combined_scores = pd.DataFrame({\n",
    "    'Feature': f_scores['Feature'],\n",
    "    'F_Score_Norm': f_scores_norm.flatten(),\n",
    "    'MI_Score_Norm': mi_scores_norm.flatten()\n",
    "})\n",
    "\n",
    "combined_scores['Combined_Score'] = (\n",
    "    0.5 * combined_scores['F_Score_Norm'] + \n",
    "    0.5 * combined_scores['MI_Score_Norm']\n",
    ")\n",
    "\n",
    "combined_scores = combined_scores.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "print(f\"\\n Top 20 Features by Combined Statistical Score:\")\n",
    "print(combined_scores.head(20)[['Feature', 'Combined_Score']])\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F-scores\n",
    "top_f = f_scores.head(20)\n",
    "axes[0].barh(range(len(top_f)), top_f['F_Score'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_f)))\n",
    "axes[0].set_yticklabels([f[:30] + '...' if len(f) > 30 else f for f in top_f['Feature']])\n",
    "axes[0].set_xlabel('F-Score')\n",
    "axes[0].set_title('Top 20 Features - F-Statistic', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MI scores\n",
    "top_mi = mi_scores.head(20)\n",
    "axes[1].barh(range(len(top_mi)), top_mi['MI_Score'], color='coral', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_mi)))\n",
    "axes[1].set_yticklabels([f[:30] + '...' if len(f) > 30 else f for f in top_mi['Feature']])\n",
    "axes[1].set_xlabel('Mutual Information Score')\n",
    "axes[1].set_title('Top 20 Features - Mutual Information', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select top K features from combined score\n",
    "k_best = min(80, len(combined_scores))\n",
    "selected_features_statistical = combined_scores.head(k_best)['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_features_statistical)} features from statistical methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 4: TREE-BASED FEATURE IMPORTANCE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 4: TREE-BASED FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Work with correlation-filtered features\n",
    "X_train_tree = X_train[selected_features_corr].copy()\n",
    "X_test_tree = X_test[selected_features_corr].copy()\n",
    "\n",
    "# Method 4A: Random Forest\n",
    "print(f\"\\n Method 4A: Random Forest Feature Importance...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_tree, y_train)\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train_tree.columns,\n",
    "    'RF_Importance': rf_model.feature_importances_\n",
    "}).sort_values('RF_Importance', ascending=False)\n",
    "\n",
    "print(f\" Random Forest completed!\")\n",
    "print(f\"\\n Top 15 Features by RF Importance:\")\n",
    "print(rf_importance.head(15))\n",
    "\n",
    "# Method 4B: Gradient Boosting\n",
    "print(f\"\\n Method 4B: Gradient Boosting Feature Importance...\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_tree, y_train)\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': X_train_tree.columns,\n",
    "    'GB_Importance': gb_model.feature_importances_\n",
    "}).sort_values('GB_Importance', ascending=False)\n",
    "\n",
    "print(f\" Gradient Boosting completed!\")\n",
    "print(f\"\\n Top 15 Features by GB Importance:\")\n",
    "print(gb_importance.head(15))\n",
    "\n",
    "# Method 4C: XGBoost\n",
    "print(f\"\\n Method 4C: XGBoost Feature Importance...\")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_tree, y_train)\n",
    "\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X_train_tree.columns,\n",
    "    'XGB_Importance': xgb_model.feature_importances_\n",
    "}).sort_values('XGB_Importance', ascending=False)\n",
    "\n",
    "print(f\" XGBoost completed!\")\n",
    "print(f\"\\n Top 15 Features by XGB Importance:\")\n",
    "print(xgb_importance.head(15))\n",
    "\n",
    "# Combine tree-based importances\n",
    "tree_importance_combined = rf_importance.merge(gb_importance, on='Feature')\n",
    "tree_importance_combined = tree_importance_combined.merge(xgb_importance, on='Feature')\n",
    "\n",
    "# Normalize and average\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "tree_importance_combined['RF_Norm'] = scaler.fit_transform(\n",
    "    tree_importance_combined[['RF_Importance']]\n",
    ")\n",
    "tree_importance_combined['GB_Norm'] = scaler.fit_transform(\n",
    "    tree_importance_combined[['GB_Importance']]\n",
    ")\n",
    "tree_importance_combined['XGB_Norm'] = scaler.fit_transform(\n",
    "    tree_importance_combined[['XGB_Importance']]\n",
    ")\n",
    "\n",
    "# Average importance\n",
    "tree_importance_combined['Avg_Tree_Importance'] = (\n",
    "    tree_importance_combined['RF_Norm'] +\n",
    "    tree_importance_combined['GB_Norm'] +\n",
    "    tree_importance_combined['XGB_Norm']\n",
    ") / 3\n",
    "\n",
    "tree_importance_combined = tree_importance_combined.sort_values(\n",
    "    'Avg_Tree_Importance', ascending=False\n",
    ")\n",
    "\n",
    "print(f\"\\n Top 20 Features by Average Tree Importance:\")\n",
    "print(tree_importance_combined.head(20)[['Feature', 'Avg_Tree_Importance']])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Random Forest\n",
    "top_rf = rf_importance.head(20)\n",
    "axes[0].barh(range(len(top_rf)), top_rf['RF_Importance'], color='forestgreen', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top_rf)))\n",
    "axes[0].set_yticklabels([f[:25] + '...' if len(f) > 25 else f for f in top_rf['Feature']], fontsize=8)\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest - Top 20', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Gradient Boosting\n",
    "top_gb = gb_importance.head(20)\n",
    "axes[1].barh(range(len(top_gb)), top_gb['GB_Importance'], color='orange', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_gb)))\n",
    "axes[1].set_yticklabels([f[:25] + '...' if len(f) > 25 else f for f in top_gb['Feature']], fontsize=8)\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Gradient Boosting - Top 20', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# XGBoost\n",
    "top_xgb = xgb_importance.head(20)\n",
    "axes[2].barh(range(len(top_xgb)), top_xgb['XGB_Importance'], color='purple', edgecolor='black')\n",
    "axes[2].set_yticks(range(len(top_xgb)))\n",
    "axes[2].set_yticklabels([f[:25] + '...' if len(f) > 25 else f for f in top_xgb['Feature']], fontsize=8)\n",
    "axes[2].set_xlabel('Importance')\n",
    "axes[2].set_title('XGBoost - Top 20', fontweight='bold')\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select top features from tree models\n",
    "k_tree = min(70, len(tree_importance_combined))\n",
    "selected_features_tree = tree_importance_combined.head(k_tree)['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n Selected {len(selected_features_tree)} features from tree-based methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 20: COMBINE ALL FEATURE SELECTION METHODS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMBINING ALL FEATURE SELECTION METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Summary of Feature Selection Methods:\")\n",
    "print(f\"   1. Variance Threshold: {len(selected_features_var)} features\")\n",
    "print(f\"   2. Correlation Filter: {len(selected_features_corr)} features\")\n",
    "print(f\"   3. Statistical (F-stat + MI): {len(selected_features_statistical)} features\")\n",
    "print(f\"   4. Tree-Based (RF + GB + XGB): {len(selected_features_tree)} features\")\n",
    "\n",
    "# Create a comprehensive scoring system\n",
    "print(f\"\\n Creating comprehensive feature scoring system...\")\n",
    "\n",
    "# Initialize score dataframe\n",
    "all_features_scores = pd.DataFrame({'Feature': selected_features_corr})\n",
    "\n",
    "# Add scores from each method\n",
    "\n",
    "# 1. Statistical scores (normalized)\n",
    "statistical_scores_dict = dict(zip(\n",
    "    combined_scores['Feature'],\n",
    "    combined_scores['Combined_Score']\n",
    "))\n",
    "all_features_scores['Statistical_Score'] = all_features_scores['Feature'].map(\n",
    "    statistical_scores_dict\n",
    ").fillna(0)\n",
    "\n",
    "# 2. Tree-based scores (normalized)\n",
    "tree_scores_dict = dict(zip(\n",
    "    tree_importance_combined['Feature'],\n",
    "    tree_importance_combined['Avg_Tree_Importance']\n",
    "))\n",
    "all_features_scores['Tree_Score'] = all_features_scores['Feature'].map(\n",
    "    tree_scores_dict\n",
    ").fillna(0)\n",
    "\n",
    "# 3. Selection frequency (how many methods selected this feature)\n",
    "all_features_scores['Selection_Count'] = all_features_scores['Feature'].apply(\n",
    "    lambda x: sum([\n",
    "        x in selected_features_var,\n",
    "        x in selected_features_corr,\n",
    "        x in selected_features_statistical,\n",
    "        x in selected_features_tree\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 4. Calculate final composite score\n",
    "# Weight: 40% tree-based, 40% statistical, 20% selection frequency\n",
    "all_features_scores['Final_Score'] = (\n",
    "    0.40 * all_features_scores['Tree_Score'] +\n",
    "    0.40 * all_features_scores['Statistical_Score'] +\n",
    "    0.20 * (all_features_scores['Selection_Count'] / 4)\n",
    ")\n",
    "\n",
    "# Sort by final score\n",
    "all_features_scores = all_features_scores.sort_values('Final_Score', ascending=False)\n",
    "\n",
    "print(f\"\\n Feature scoring completed!\")\n",
    "print(f\"\\n Top 30 Features by Final Composite Score:\")\n",
    "print(all_features_scores.head(30))\n",
    "\n",
    "# Visualize top features\n",
    "top_features_final = all_features_scores.head(30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Bar plot of final scores\n",
    "axes[0].barh(range(len(top_features_final)), top_features_final['Final_Score'], \n",
    "             color='teal', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_features_final)))\n",
    "axes[0].set_yticklabels([f[:35] + '...' if len(f) > 35 else f \n",
    "                         for f in top_features_final['Feature']], fontsize=9)\n",
    "axes[0].set_xlabel('Final Composite Score', fontsize=11)\n",
    "axes[0].set_title('Top 30 Features - Final Score', fontweight='bold', fontsize=13)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Stacked scores breakdown\n",
    "top_20 = all_features_scores.head(20)\n",
    "x_pos = np.arange(len(top_20))\n",
    "\n",
    "axes[1].barh(x_pos, top_20['Tree_Score'], \n",
    "            label='Tree-Based (40%)', color='forestgreen', alpha=0.8)\n",
    "axes[1].barh(x_pos, top_20['Statistical_Score'], \n",
    "            left=top_20['Tree_Score'],\n",
    "            label='Statistical (40%)', color='steelblue', alpha=0.8)\n",
    "axes[1].barh(x_pos, top_20['Selection_Count']/4 * 0.5,\n",
    "            left=top_20['Tree_Score'] + top_20['Statistical_Score'],\n",
    "            label='Selection Freq (20%)', color='coral', alpha=0.8)\n",
    "\n",
    "axes[1].set_yticks(x_pos)\n",
    "axes[1].set_yticklabels([f[:30] + '...' if len(f) > 30 else f \n",
    "                         for f in top_20['Feature']], fontsize=8)\n",
    "axes[1].set_xlabel('Score Contribution', fontsize=11)\n",
    "axes[1].set_title('Top 20 Features - Score Breakdown', fontweight='bold', fontsize=13)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 21: RECURSIVE FEATURE ELIMINATION (RFECV)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECURSIVE FEATURE ELIMINATION WITH CROSS-VALIDATION (RFECV)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Use top 100 features from composite scoring\n",
    "top_100_features = all_features_scores.head(100)['Feature'].tolist()\n",
    "\n",
    "X_train_rfecv = X_train[top_100_features].copy()\n",
    "X_test_rfecv = X_test[top_100_features].copy()\n",
    "\n",
    "print(f\"\\n Running RFECV on top 100 features...\")\n",
    "print(f\"   This may take a few minutes...\")\n",
    "\n",
    "# Use Ridge regression for RFECV (faster than tree models)\n",
    "estimator = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# RFECV with 5-fold cross-validation\n",
    "rfecv = RFECV(\n",
    "    estimator=estimator,\n",
    "    step=5,  # Remove 5 features at a time\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train_rfecv, y_train)\n",
    "\n",
    "print(f\"\\n RFECV completed!\")\n",
    "print(f\"   Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"   Features eliminated: {len(top_100_features) - rfecv.n_features_}\")\n",
    "\n",
    "# Get selected features\n",
    "selected_features_rfecv = X_train_rfecv.columns[rfecv.support_].tolist()\n",
    "\n",
    "print(f\"\\n RFECV Selected Features ({len(selected_features_rfecv)}):\")\n",
    "for i, feature in enumerate(selected_features_rfecv[:20], 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "if len(selected_features_rfecv) > 20:\n",
    "    print(f\"   ... and {len(selected_features_rfecv) - 20} more\")\n",
    "\n",
    "# Plot cross-validation scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n",
    "         -rfecv.cv_results_['mean_test_score'],\n",
    "         marker='o', linewidth=2, markersize=6, color='darkblue')\n",
    "plt.axvline(x=rfecv.n_features_, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Optimal: {rfecv.n_features_} features')\n",
    "plt.xlabel('Number of Features Selected', fontsize=12)\n",
    "plt.ylabel('Cross-Validation MSE', fontsize=12)\n",
    "plt.title('RFECV - Feature Selection Performance', fontweight='bold', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 22: FINAL FEATURE SET SELECTION (ENSEMBLE VOTING)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL FEATURE SET SELECTION - ENSEMBLE VOTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create voting system across all methods\n",
    "print(f\"\\n Applying ensemble voting across all selection methods...\")\n",
    "\n",
    "# Get top N from each method\n",
    "top_k = 60  # Take top 60 from each method\n",
    "\n",
    "top_statistical = combined_scores.head(top_k)['Feature'].tolist()\n",
    "top_tree = tree_importance_combined.head(top_k)['Feature'].tolist()\n",
    "top_rfecv = selected_features_rfecv\n",
    "\n",
    "# Count votes for each feature\n",
    "feature_votes = {}\n",
    "\n",
    "for feature in selected_features_corr:\n",
    "    votes = 0\n",
    "    \n",
    "    # Vote from statistical methods\n",
    "    if feature in top_statistical:\n",
    "        rank = top_statistical.index(feature) + 1\n",
    "        votes += (top_k - rank + 1) / top_k  # Higher rank = more votes\n",
    "    \n",
    "    # Vote from tree-based methods\n",
    "    if feature in top_tree:\n",
    "        rank = top_tree.index(feature) + 1\n",
    "        votes += (top_k - rank + 1) / top_k\n",
    "    \n",
    "    # Vote from RFECV\n",
    "    if feature in top_rfecv:\n",
    "        votes += 1.5  # RFECV gets extra weight (validated through CV)\n",
    "    \n",
    "    feature_votes[feature] = votes\n",
    "\n",
    "# Create voting dataframe\n",
    "voting_df = pd.DataFrame({\n",
    "    'Feature': list(feature_votes.keys()),\n",
    "    'Votes': list(feature_votes.values())\n",
    "}).sort_values('Votes', ascending=False)\n",
    "\n",
    "# Add method indicators\n",
    "voting_df['In_Statistical_Top60'] = voting_df['Feature'].isin(top_statistical)\n",
    "voting_df['In_Tree_Top60'] = voting_df['Feature'].isin(top_tree)\n",
    "voting_df['In_RFECV'] = voting_df['Feature'].isin(top_rfecv)\n",
    "voting_df['Method_Count'] = (\n",
    "    voting_df['In_Statistical_Top60'].astype(int) +\n",
    "    voting_df['In_Tree_Top60'].astype(int) +\n",
    "    voting_df['In_RFECV'].astype(int)\n",
    ")\n",
    "\n",
    "print(f\"\\n Voting completed!\")\n",
    "print(f\"\\n Top 40 Features by Ensemble Voting:\")\n",
    "print(voting_df.head(40))\n",
    "\n",
    "# Visualize voting results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Top features by votes\n",
    "top_30_votes = voting_df.head(30)\n",
    "axes[0].barh(range(len(top_30_votes)), top_30_votes['Votes'],\n",
    "            color='darkgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_30_votes)))\n",
    "axes[0].set_yticklabels([f[:35] + '...' if len(f) > 35 else f \n",
    "                        for f in top_30_votes['Feature']], fontsize=9)\n",
    "axes[0].set_xlabel('Vote Score', fontsize=11)\n",
    "axes[0].set_title('Top 30 Features - Ensemble Voting', fontweight='bold', fontsize=13)\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Method agreement\n",
    "method_counts = voting_df['Method_Count'].value_counts().sort_index()\n",
    "axes[1].bar(method_counts.index, method_counts.values,\n",
    "           color=['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4'],\n",
    "           edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Methods Agreeing', fontsize=11)\n",
    "axes[1].set_ylabel('Number of Features', fontsize=11)\n",
    "axes[1].set_title('Feature Selection Method Agreement', fontweight='bold', fontsize=13)\n",
    "axes[1].set_xticks([0, 1, 2, 3])\n",
    "axes[1].set_xticklabels(['0 methods', '1 method', '2 methods', '3 methods'])\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add text annotations\n",
    "for i, count in enumerate(method_counts.values):\n",
    "    axes[1].text(method_counts.index[i], count + 1, str(count),\n",
    "                ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select final features (threshold: at least 2 methods agree OR top 50 by votes)\n",
    "final_features_threshold = voting_df[voting_df['Method_Count'] >= 2]['Feature'].tolist()\n",
    "final_features_top = voting_df.head(60)['Feature'].tolist()\n",
    "\n",
    "# Combine both criteria\n",
    "final_selected_features = list(set(final_features_threshold + final_features_top))\n",
    "\n",
    "print(f\"\\n Final Feature Selection:\")\n",
    "print(f\"   Features selected by ≥2 methods: {len(final_features_threshold)}\")\n",
    "print(f\"   Top 60 by voting: {len(final_features_top)}\")\n",
    "print(f\"   Total unique features: {len(final_selected_features)}\")\n",
    "\n",
    "print(f\"\\n Final Selected Features ({len(final_selected_features)}):\")\n",
    "final_features_sorted = voting_df[voting_df['Feature'].isin(final_selected_features)].copy()\n",
    "final_features_sorted = final_features_sorted.sort_values('Votes', ascending=False)\n",
    "\n",
    "for i, row in enumerate(final_features_sorted.head(30).itertuples(), 1):\n",
    "    methods = []\n",
    "    if row.In_Statistical_Top60:\n",
    "        methods.append('Stat')\n",
    "    if row.In_Tree_Top60:\n",
    "        methods.append('Tree')\n",
    "    if row.In_RFECV:\n",
    "        methods.append('RFECV')\n",
    "    \n",
    "    print(f\"   {i:2d}. {row.Feature[:50]:<50} | Votes: {row.Votes:.2f} | [{', '.join(methods)}]\")\n",
    "\n",
    "if len(final_features_sorted) > 30:\n",
    "    print(f\"   ... and {len(final_features_sorted) - 30} more features\")\n",
    "\n",
    "# Save final feature list\n",
    "final_features_list = final_features_sorted['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 23: ANALYZE FINAL FEATURE SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING FINAL FEATURE SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Categorize features by type\n",
    "feature_categories = {\n",
    "    'Time Series': [],\n",
    "    'Temporal': [],\n",
    "    'Weather': [],\n",
    "    'Pollutants': [],\n",
    "    'Location Aggregations': [],\n",
    "    'Wind Features': [],\n",
    "    'Interactions': [],\n",
    "    'Ratios': [],\n",
    "    'Polynomial': [],\n",
    "    'PCA': [],\n",
    "    'Clustering': [],\n",
    "    'Other': []\n",
    "}\n",
    "\n",
    "for feature in final_features_list:\n",
    "    if 'rolling' in feature or 'lag' in feature or 'diff' in feature:\n",
    "        feature_categories['Time Series'].append(feature)\n",
    "    elif 'month' in feature or 'day' in feature or 'season' in feature or 'weekend' in feature:\n",
    "        feature_categories['Temporal'].append(feature)\n",
    "    elif 'temperature' in feature or 'humidity' in feature or 'precipitable' in feature:\n",
    "        feature_categories['Weather'].append(feature)\n",
    "    elif 'L3_' in feature and any(x in feature for x in ['NO2', 'CO', 'SO2', 'HCHO', 'O3']):\n",
    "        feature_categories['Pollutants'].append(feature)\n",
    "    elif 'place_' in feature:\n",
    "        feature_categories['Location Aggregations'].append(feature)\n",
    "    elif 'wind' in feature.lower():\n",
    "        feature_categories['Wind Features'].append(feature)\n",
    "    elif 'interaction' in feature or 'pollutant_load' in feature or 'AQI' in feature:\n",
    "        feature_categories['Interactions'].append(feature)\n",
    "    elif 'ratio' in feature:\n",
    "        feature_categories['Ratios'].append(feature)\n",
    "    elif 'squared' in feature or 'sqrt' in feature or 'log' in feature:\n",
    "        feature_categories['Polynomial'].append(feature)\n",
    "    elif 'pca' in feature.lower():\n",
    "        feature_categories['PCA'].append(feature)\n",
    "    elif 'cluster' in feature:\n",
    "        feature_categories['Clustering'].append(feature)\n",
    "    else:\n",
    "        feature_categories['Other'].append(feature)\n",
    "\n",
    "print(f\"\\n Feature Breakdown by Category:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_check = 0\n",
    "for category, features in feature_categories.items():\n",
    "    if len(features) > 0:\n",
    "        print(f\"\\n{category}: {len(features)} features\")\n",
    "        for feature in features[:5]:\n",
    "            print(f\"   - {feature}\")\n",
    "        if len(features) > 5:\n",
    "            print(f\"   ... and {len(features) - 5} more\")\n",
    "        total_check += len(features)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total features: {total_check}\")\n",
    "\n",
    "# Visualize category distribution\n",
    "category_counts = {k: len(v) for k, v in feature_categories.items() if len(v) > 0}\n",
    "category_counts = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(category_counts)), list(category_counts.values()),\n",
    "       color='skyblue', edgecolor='black', alpha=0.8)\n",
    "plt.xticks(range(len(category_counts)), list(category_counts.keys()), rotation=45, ha='right')\n",
    "plt.xlabel('Feature Category', fontsize=12)\n",
    "plt.ylabel('Number of Features', fontsize=12)\n",
    "plt.title('Final Feature Set - Category Distribution', fontweight='bold', fontsize=14)\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate(category_counts.values()):\n",
    "    plt.text(i, count + 0.5, str(count), ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Feature analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 24: PREPARE FINAL DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING FINAL DATASET FOR MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create final dataset with selected features\n",
    "print(f\"\\n Creating final dataset with {len(final_features_list)} selected features...\")\n",
    "\n",
    "# Features for modeling\n",
    "X_final = df_fe[final_features_list].copy()\n",
    "y_final = df_fe['target_log'].copy()  # Using log-transformed target\n",
    "y_original = df_fe['target'].copy()   # Keep original for reference\n",
    "\n",
    "# Metadata to keep\n",
    "metadata_cols = ['Date', 'Place_ID']\n",
    "metadata = df_fe[metadata_cols].copy()\n",
    "\n",
    "print(f\"\\n Final dataset prepared!\")\n",
    "print(f\"   Features: {X_final.shape[1]}\")\n",
    "print(f\"   Samples: {X_final.shape[0]}\")\n",
    "print(f\"   Target: log-transformed PM2.5\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_final = X_final.isnull().sum()\n",
    "if missing_final > 0:\n",
    "    print(f\"\\n Warning: {missing_final} missing values remaining\")\n",
    "    print(f\"   Filling with median...\")\n",
    "    X_final = X_final.fillna(X_final.median())\n",
    "    print(f\"    Missing values handled\")\n",
    "else:\n",
    "    print(f\"\\n No missing values!\")\n",
    "\n",
    "# Train-test split\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Also split metadata and original target\n",
    "metadata_train, metadata_test = train_test_split(\n",
    "    metadata, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "y_train_original, y_test_original = train_test_split(\n",
    "    y_original, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n Train-Test Split:\")\n",
    "print(f\"   Training set: {X_train_final.shape}\")\n",
    "print(f\"   Test set: {X_test_final.shape}\")\n",
    "print(f\"   Split ratio: 80-20\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n Final Dataset Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total features: {X_final.shape[1]}\")\n",
    "print(f\"Total samples: {X_final.shape[0]}\")\n",
    "print(f\"Training samples: {X_train_final.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_final.shape[0]}\")\n",
    "print(f\"\\nTarget variable (log-transformed):\")\n",
    "print(f\"   Mean: {y_final.mean():.3f}\")\n",
    "print(f\"   Std: {y_final.std():.3f}\")\n",
    "print(f\"   Min: {y_final.min():.3f}\")\n",
    "print(f\"   Max: {y_final.max():.3f}\")\n",
    "print(f\"   Skewness: {y_final.skew():.3f}\")\n",
    "\n",
    "# Visualize final feature importance across all selected features\n",
    "print(f\"\\n📊 Generating final feature importance visualization...\")\n",
    "\n",
    "# Get importance scores for final features\n",
    "final_feature_scores = voting_df[voting_df['Feature'].isin(final_features_list)].copy()\n",
    "final_feature_scores = final_feature_scores.sort_values('Votes', ascending=False).head(40)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.barh(range(len(final_feature_scores)), final_feature_scores['Votes'],\n",
    "        color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "plt.yticks(range(len(final_feature_scores)),\n",
    "          [f[:40] + '...' if len(f) > 40 else f for f in final_feature_scores['Feature']],\n",
    "          fontsize=9)\n",
    "plt.xlabel('Vote Score', fontsize=12)\n",
    "plt.title('Top 40 Features in Final Selection - Vote Scores', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 25: SAVE PROCESSED DATA AND ARTIFACTS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING PROCESSED DATA AND ARTIFACTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../artifacts', exist_ok=True)\n",
    "os.makedirs('../artifacts/feature_selection', exist_ok=True)\n",
    "\n",
    "print(f\"\\n Saving datasets...\")\n",
    "\n",
    "# 1. Save train and test sets\n",
    "X_train_final.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test_final.to_csv('../data/processed/X_test.csv', index=False)\n",
    "\n",
    "y_train_final.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test_final.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save original targets too\n",
    "y_train_original.to_csv('../data/processed/y_train_original.csv', index=False)\n",
    "y_test_original.to_csv('../data/processed/y_test_original.csv', index=False)\n",
    "\n",
    "# Save metadata\n",
    "metadata_train.to_csv('../data/processed/metadata_train.csv', index=False)\n",
    "metadata_test.to_csv('../data/processed/metadata_test.csv', index=False)\n",
    "\n",
    "print(f\"    Saved train/test datasets\")\n",
    "\n",
    "# 2. Save feature lists\n",
    "with open('../artifacts/feature_selection/final_features.txt', 'w') as f:\n",
    "    for feature in final_features_list:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"    Saved final feature list ({len(final_features_list)} features)\")\n",
    "\n",
    "# 3. Save feature scores\n",
    "voting_df.to_csv('../artifacts/feature_selection/feature_voting_scores.csv', index=False)\n",
    "all_features_scores.to_csv('../artifacts/feature_selection/all_feature_scores.csv', index=False)\n",
    "tree_importance_combined.to_csv('../artifacts/feature_selection/tree_importance.csv', index=False)\n",
    "combined_scores.to_csv('../artifacts/feature_selection/statistical_scores.csv', index=False)\n",
    "\n",
    "print(f\"    Saved feature importance scores\")\n",
    "\n",
    "# 4. Save feature categories\n",
    "with open('../artifacts/feature_selection/feature_categories.txt', 'w') as f:\n",
    "    for category, features in feature_categories.items():\n",
    "        if len(features) > 0:\n",
    "            f.write(f\"\\n{category} ({len(features)} features):\\n\")\n",
    "            for feature in features:\n",
    "                f.write(f\"  - {feature}\\n\")\n",
    "\n",
    "print(f\"    Saved feature categories\")\n",
    "\n",
    "# 5. Save complete engineered dataset (before feature selection)\n",
    "df_fe.to_csv('../data/processed/data_fully_engineered.csv', index=False)\n",
    "\n",
    "print(f\"    Saved complete engineered dataset\")\n",
    "\n",
    "# 6. Create and save feature engineering summary\n",
    "summary = {\n",
    "    'original_features': len(df.columns),\n",
    "    'engineered_features': len(df_fe.columns),\n",
    "    'final_selected_features': len(final_features_list),\n",
    "    'train_samples': len(X_train_final),\n",
    "    'test_samples': len(X_test_final),\n",
    "    'target_transformation': 'log1p',\n",
    "    'missing_value_strategy': 'KNN + Group Imputation',\n",
    "    'outlier_handling': 'IQR Capping',\n",
    "    'feature_selection_methods': [\n",
    "        'Variance Threshold',\n",
    "        'Correlation Filter',\n",
    "        'SelectKBest (F-stat + MI)',\n",
    "        'Tree-Based Importance (RF + GB + XGB)',\n",
    "        'RFECV',\n",
    "        'Ensemble Voting'\n",
    "    ]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../artifacts/feature_selection/feature_engineering_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(f\"    Saved feature engineering summary\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\" ALL DATA AND ARTIFACTS SAVED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n Saved Files:\")\n",
    "print(f\"   ../data/processed/\")\n",
    "print(f\"      - X_train.csv ({X_train_final.shape})\")\n",
    "print(f\"      - X_test.csv ({X_test_final.shape})\")\n",
    "print(f\"      - y_train.csv\")\n",
    "print(f\"      - y_test.csv\")\n",
    "print(f\"      - y_train_original.csv\")\n",
    "print(f\"      - y_test_original.csv\")\n",
    "print(f\"      - metadata_train.csv\")\n",
    "print(f\"      - metadata_test.csv\")\n",
    "print(f\"      - data_fully_engineered.csv ({df_fe.shape})\")\n",
    "print(f\"\\n   ../artifacts/feature_selection/\")\n",
    "print(f\"      - final_features.txt\")\n",
    "print(f\"      - feature_voting_scores.csv\")\n",
    "print(f\"      - all_feature_scores.csv\")\n",
    "print(f\"      - tree_importance.csv\")\n",
    "print(f\"      - statistical_scores.csv\")\n",
    "print(f\"      - feature_categories.txt\")\n",
    "print(f\"      - feature_engineering_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
