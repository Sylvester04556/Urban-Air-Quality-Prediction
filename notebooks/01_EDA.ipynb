{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Air Quality PM2.5 Prediction - Exploratory Data Analysis\n",
    "\n",
    "**Author:** Chukwuahachie Sylvester  \n",
    "**Date:** December 1, 2025  \n",
    "**Purpose:** Initial exploration of air quality dataset\n",
    "\n",
    "## Objectives\n",
    "1. Understand the dataset structure\n",
    "2. Identify data quality issues (missing values, outliers)\n",
    "3. Analyze feature distributions\n",
    "4. Explore relationships between features and target (PM2.5)\n",
    "5. Generate insights for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/raw/air_quality.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LAST 5 ROWS\")\n",
    "print(\"=\" * 60)\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM SAMPLE (5 ROWS)\")\n",
    "print(\"=\" * 60)\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Information\n",
    "print('=' * 60)\n",
    "print('DATASET INFORMATION')\n",
    "print('=' * 60)\n",
    "display(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('MEMORY USAGE')\n",
    "print('=' * 60)\n",
    "print(f\"Total memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical Summary\n",
    "print('=' * 60)\n",
    "print('STASTISTICAL SUMMARY - NUMERICAL FEATURES')\n",
    "print('=' * 60)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY - CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "display(df.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "\n",
    "# Create summary dataframe\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_percent.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Display only columns with missing values\n",
    "missing_df_filtered = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_df_filtered) > 0:\n",
    "    print(\"\\n Columns with Missing Values:\")\n",
    "    display(missing_df_filtered)\n",
    "else:\n",
    "    print(\"\\n No missing values found!\")\n",
    "\n",
    "# Visualize missing values\n",
    "if missing.sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=True, cmap='viridis', yticklabels=False)\n",
    "    plt.title('Missing Values Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n No missing values to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TARGET VARIABLE ANALYSIS (PM2.5)\n",
    "# ============================================\n",
    "\n",
    "# Define your target variable name\n",
    "TARGET = 'target'  \n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"TARGET VARIABLE ANALYSIS: {TARGET}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if TARGET in df.columns:\n",
    "    print(f\"\\n Summary Statistics for {TARGET}:\")\n",
    "    print(df[TARGET].describe())\n",
    "    \n",
    "    print(f\"\\n Additional Statistics:\")\n",
    "    print(f\"Skewness: {df[TARGET].skew():.3f}\")\n",
    "    print(f\"Kurtosis: {df[TARGET].kurtosis():.3f}\")\n",
    "    print(f\"Range: {df[TARGET].min():.2f} - {df[TARGET].max():.2f}\")\n",
    "    print(f\"IQR: {df[TARGET].quantile(0.75) - df[TARGET].quantile(0.25):.2f}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Histogram with KDE\n",
    "    axes[0, 0].hist(df[TARGET].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel(TARGET)\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title(f'Distribution of {TARGET}', fontweight='bold')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot\n",
    "    axes[0, 1].boxplot(df[TARGET].dropna(), vert=True)\n",
    "    axes[0, 1].set_ylabel(TARGET)\n",
    "    axes[0, 1].set_title(f'Box Plot of {TARGET}', fontweight='bold')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. KDE plot\n",
    "    df[TARGET].dropna().plot(kind='kde', ax=axes[1, 0], linewidth=2)\n",
    "    axes[1, 0].set_xlabel(TARGET)\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title(f'Kernel Density Plot of {TARGET}', fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q plot\n",
    "    stats.probplot(df[TARGET].dropna(), dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Q-Q Plot of {TARGET}', fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE TYPE IDENTIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# Remove target from numerical if present\n",
    "if TARGET in numerical_cols:\n",
    "    numerical_cols.remove(TARGET)\n",
    "\n",
    "print(f\"\\n Numerical Features ({len(numerical_cols)}):\")\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(f\"\\n Categorical Features ({len(categorical_cols)}):\")\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    print(f\"   {i}. {col} - {df[col].nunique()} unique values\")\n",
    "\n",
    "if datetime_cols:\n",
    "    print(f\"\\n DateTime Features ({len(datetime_cols)}):\")\n",
    "    for i, col in enumerate(datetime_cols, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "\n",
    "print(f\"\\n Target Variable: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NUMERICAL FEATURES - DISTRIBUTION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nAnalyzing {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Calculate statistics for all numerical features\n",
    "stats_summary = pd.DataFrame({\n",
    "    'Mean': df[numerical_cols].mean(),\n",
    "    'Median': df[numerical_cols].median(),\n",
    "    'Std': df[numerical_cols].std(),\n",
    "    'Min': df[numerical_cols].min(),\n",
    "    'Max': df[numerical_cols].max(),\n",
    "    'Skewness': df[numerical_cols].skew(),\n",
    "    'Missing_%': (df[numerical_cols].isnull().sum() / len(df) * 100)\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n Top 10 Most Variable Features (by Std Dev):\")\n",
    "display(stats_summary.nlargest(10, 'Std')[['Mean', 'Std', 'Skewness', 'Missing_%']])\n",
    "\n",
    "print(\"\\n Top 10 Most Skewed Features:\")\n",
    "display(stats_summary.nlargest(10, 'Skewness')[['Mean', 'Skewness', 'Missing_%']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WEATHER FEATURES ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WEATHER FEATURES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define weather features \n",
    "weather_features = [\n",
    "    'precipitable_water_entire_atmosphere',\n",
    "    'relative_humidity_2m_above_ground',\n",
    "    'specific_humidity_2m_above_ground',\n",
    "    'temperature_2m_above_ground',\n",
    "    'u_component_of_wind_10m_above_ground',\n",
    "    'v_component_of_wind_10m_above_ground'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "weather_features = [col for col in weather_features if col in df.columns]\n",
    "\n",
    "print(f\"\\n Analyzing {len(weather_features)} weather features\")\n",
    "\n",
    "# Create subplots\n",
    "n_features = len(weather_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4))\n",
    "axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(weather_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    df[feature].dropna().hist(bins=50, ax=ax, alpha=0.7, edgecolor='black')\n",
    "    df[feature].dropna().plot(kind='kde', ax=ax, secondary_y=True, color='red', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution: {feature[:30]}...', fontweight='bold', fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f\"Mean: {df[feature].mean():.2f}\\nStd: {df[feature].std():.2f}\\nSkew: {df[feature].skew():.2f}\"\n",
    "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "            fontsize=8)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df.corr(numeric_only = True)\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_matrix[TARGET].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n Top 15 Features Most Correlated with Target:\")\n",
    "print(target_corr.head(16)[1:])  \n",
    "\n",
    "print(\"\\n Top 15 Features Most Negatively Correlated with Target:\")\n",
    "print(target_corr.tail(15))\n",
    "\n",
    "# Visualize correlation matrix (top correlated features only)\n",
    "top_features = target_corr.head(20).index.tolist()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(df[top_features].corr(), \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix - Top 20 Features vs Target', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OUTLIER DETECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    return len(outliers), (len(outliers) / len(data)) * 100\n",
    "\n",
    "# Check outliers in target variable\n",
    "outliers_count, outliers_pct = detect_outliers_iqr(df, 'target')\n",
    "print(f\"\\n Target Variable\")\n",
    "print(f\"   Count: {outliers_count}\")\n",
    "print(f\"   Percentage: {outliers_pct:.2f}%\")\n",
    "\n",
    "# Check outliers in key weather features\n",
    "print(f\"\\n Weather Features Outliers:\")\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in weather_features:\n",
    "    count, pct = detect_outliers_iqr(df, feature)\n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Outlier_Count': count,\n",
    "        'Outlier_Percentage': round(pct, 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Percentage', ascending=False)\n",
    "display(outlier_df)\n",
    "\n",
    "# Visualize outliers with boxplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(weather_features):\n",
    "    ax = axes[idx]\n",
    "    df.boxplot(column=feature, ax=ax, patch_artist=True)\n",
    "    ax.set_title(f'Outliers: {feature[:30]}...', fontweight='bold', fontsize=10)\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE VS TARGET RELATIONSHIPS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE VS TARGET RELATIONSHIPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get top correlated features\n",
    "top_positive_features = target_corr.head(6)[1:].index.tolist()  # Exclude target itself\n",
    "top_negative_features = target_corr.tail(5).index.tolist()\n",
    "\n",
    "print(f\"\\n Plotting relationships for top correlated features...\")\n",
    "\n",
    "# Plot positive correlations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_positive_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    valid_data = df[[feature, 'target']].dropna()\n",
    "    \n",
    "    ax.scatter(valid_data[feature], valid_data['target'], alpha=0.3, s=10)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(valid_data[feature], valid_data['target'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(valid_data[feature].sort_values(), \n",
    "            p(valid_data[feature].sort_values()), \n",
    "            \"r--\", linewidth=2, label=f'Trend')\n",
    "    \n",
    "    corr_val = df[[feature, 'target']].corr().iloc[0, 1]\n",
    "    ax.set_xlabel(feature[:40], fontsize=9)\n",
    "    ax.set_ylabel('Target (PM2.5)')\n",
    "    ax.set_title(f'{feature[:30]}...\\nCorr: {corr_val:.3f}', \n",
    "                 fontweight='bold', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot negative correlations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_negative_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    valid_data = df[[feature, 'target']].dropna()\n",
    "    \n",
    "    ax.scatter(valid_data[feature], valid_data['target'], alpha=0.3, s=10, color='orange')\n",
    "    \n",
    "    z = np.polyfit(valid_data[feature], valid_data['target'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(valid_data[feature].sort_values(), \n",
    "            p(valid_data[feature].sort_values()), \n",
    "            \"r--\", linewidth=2, label=f'Trend')\n",
    "    \n",
    "    corr_val = df[[feature, 'target']].corr().iloc[0, 1]\n",
    "    ax.set_xlabel(feature[:40], fontsize=9)\n",
    "    ax.set_ylabel('Target (PM2.5)')\n",
    "    ax.set_title(f'{feature[:30]}...\\nCorr: {corr_val:.3f}', \n",
    "                 fontweight='bold', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide extra subplot if exists\n",
    "if len(top_negative_features) < 6:\n",
    "    for idx in range(len(top_negative_features), 6):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CATEGORICAL FEATURES ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(f\"\\nAnalyzing {len(categorical_cols)} categorical features:\")\n",
    "\n",
    "for feature in categorical_cols:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Value counts\n",
    "    value_counts = df[feature].value_counts()\n",
    "    print(f\"\\nUnique values: {df[feature].nunique()}\")\n",
    "    print(f\"\\nTop 10 most frequent values:\")\n",
    "    print(value_counts.head(10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEMPORAL ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert Date column to datetime if it exists\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "    \n",
    "    print(\"\\nTemporal features extracted\")\n",
    "    \n",
    "    # Time series plot\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Daily average PM2.5\n",
    "    daily_avg = df.groupby('Date')['target'].mean()\n",
    "    plt.plot(daily_avg.index, daily_avg.values, alpha=0.6, linewidth=1)\n",
    "    \n",
    "    # Add 7-day moving average\n",
    "    plt.plot(daily_avg.index, daily_avg.rolling(window=7).mean(), \n",
    "             color='red', linewidth=2, label='7-day Moving Average')\n",
    "    \n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('PM2.5 (μg/m³)', fontsize=12)\n",
    "    plt.title('PM2.5 Concentration Over Time', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Monthly patterns\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    monthly_stats = df.groupby('Month')['target'].mean()\n",
    "    monthly_stats.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Mean Target')\n",
    "    plt.title('Average Target by Month', fontweight='bold')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    dow_stats = df.groupby('DayOfWeek')['target'].mean()\n",
    "    dow_stats.plot(kind='bar', color='coral', edgecolor='black')\n",
    "    plt.xlabel('Day of Week (0=Monday)')\n",
    "    plt.ylabel('Mean Target')\n",
    "    plt.title('Average target by Day of Week', fontweight='bold')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n No Date column found for temporal analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
